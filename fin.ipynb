{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"[라벨]경상도_학습데이터_1/DKCI20000001.json\", encoding=\"UTF-8\")\n",
    "raw_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리, 표준어 사전 생성 함수\n",
    "def make_dict(raw_data):\n",
    "\n",
    "    dialect_word = []\n",
    "    standard_word = [] \n",
    "    \n",
    "    \n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        eojeol = raw_data[\"utterance\"][i][\"eojeolList\"]\n",
    "            \n",
    "        for k in range(len(eojeol)):\n",
    "            if eojeol[k][\"isDialect\"] == True:\n",
    "                if k not in dialect_word:\n",
    "                    dialect_word.append(eojeol[k]['eojeol'])\n",
    "                    standard_word.append(eojeol[k]['standard'])\n",
    "\n",
    "    word_dictionary = pd.DataFrame([dialect_word, standard_word])\n",
    "    word_dictionary = word_dictionary.transpose()\n",
    "    word_dictionary.columns = ['dialect', 'standard']\n",
    "    dialect_dictionary = word_dictionary['dialect']\n",
    "    standard_dictionary = word_dictionary['standard']\n",
    "    dialect_dict = dialect_dictionary.to_dict()\n",
    "    standard_dict = standard_dictionary.to_dict()\n",
    "\n",
    "    index_to_dialect = {v:k for k,v in dialect_dict.items()}\n",
    "    index_to_standard = {v:k for k,v in standard_dict.items()}\n",
    "\n",
    "    dialect_to_index = {v:k for k,v in index_to_dialect.items()}\n",
    "    standard_to_index = {v:k for k,v in index_to_standard.items()}\n",
    "\n",
    "    print(len(index_to_dialect))\n",
    "    print(len(dialect_to_index))\n",
    "\n",
    "    print(len(index_to_standard))\n",
    "    print(len(standard_to_index))\n",
    "    \n",
    "\n",
    "    return dialect_to_index,standard_to_index,index_to_dialect,index_to_standard\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "236\n",
      "223\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "index_to_dialect,index_to_standard,dialect_to_index,standard_to_index = make_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 표준어 쌍 생성 함수\n",
    "def make_pair(raw_data):\n",
    "    filter = re.compile('[^가-힣+]')\n",
    "\n",
    "    dialect_form = []\n",
    "    standard_form = []\n",
    "\n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        sentence = raw_data[\"utterance\"][i]\n",
    "        \n",
    "        for j in range(len(sentence[\"eojeolList\"])):\n",
    "            #print(sentence[\"eojeolList\"][0]['eojeol'])\n",
    "            if sentence[\"eojeolList\"][j]['isDialect'] == True:\n",
    "                try : \n",
    "\n",
    "                    dialect = sentence[\"eojeolList\"][j][\"eojeol\"]\n",
    "                    standard = sentence[\"eojeolList\"][j][\"standard\"]\n",
    "                    \n",
    "                    dialect = filter.sub('', dialect)\n",
    "                    standard = filter.sub('', standard)\n",
    "                    \n",
    "                    dialect = dialect_to_index[dialect]\n",
    "                    standard = standard_to_index[standard]\n",
    "\n",
    "                    dialect_form.append(dialect)\n",
    "                    standard_form.append(standard)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    result = pd.DataFrame(data = {\"dialect_words\":dialect_form, \"standard_words\": standard_form})\n",
    "\n",
    "\n",
    "    return result\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = make_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 사전에 존재하는지 확인하는 함수\n",
    "\n",
    "def exist_in_dialect(x):\n",
    "\n",
    "    if x in dialect_to_index:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exist_in_dialect(\"자가격리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = np_utils.to_categorical(pairs[\"dialect_words\"])\n",
    "ytrain = np_utils.to_categorical(pairs[\"standard_words\"])\n",
    "len(xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 416)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model = Sequential()\n",
    "translate_model.add(Dense(256, input_dim = 416, activation=\"relu\"))\n",
    "translate_model.add(Dense(416, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 6.0181 - accuracy: 0.0377\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.9630 - accuracy: 0.2588\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 5.9087 - accuracy: 0.3844\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.8511 - accuracy: 0.4724\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.7877 - accuracy: 0.4899\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.7174 - accuracy: 0.5176\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.6391 - accuracy: 0.5025\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.5511 - accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.4523 - accuracy: 0.4950\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.3420 - accuracy: 0.4824\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.2182 - accuracy: 0.4774\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.0830 - accuracy: 0.4598\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.9351 - accuracy: 0.4523\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 4.7702 - accuracy: 0.4347\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.5973 - accuracy: 0.4196\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 4.4125 - accuracy: 0.4045\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.2166 - accuracy: 0.3844\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.0148 - accuracy: 0.3819\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.8138 - accuracy: 0.3744\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.6122 - accuracy: 0.3693\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.4168 - accuracy: 0.3668\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2352 - accuracy: 0.3668\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0674 - accuracy: 0.3668\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9142 - accuracy: 0.3744\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.7768 - accuracy: 0.3744\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6524 - accuracy: 0.4045\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5327 - accuracy: 0.4372\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.4211 - accuracy: 0.4598\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3139 - accuracy: 0.4774\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2108 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.1090 - accuracy: 0.5075\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0146 - accuracy: 0.5377\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9194 - accuracy: 0.5829\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8264 - accuracy: 0.6633\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.7366 - accuracy: 0.7688\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.6478 - accuracy: 0.8518\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5606 - accuracy: 0.9045\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4746 - accuracy: 0.9347\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3923 - accuracy: 0.9497\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3114 - accuracy: 0.9523\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2321 - accuracy: 0.9523\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1544 - accuracy: 0.9523\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0779 - accuracy: 0.9523\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0062 - accuracy: 0.9523\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9369 - accuracy: 0.9523\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8714 - accuracy: 0.9523\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8055 - accuracy: 0.9523\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7463 - accuracy: 0.9523\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.9523\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6353 - accuracy: 0.9523\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5859 - accuracy: 0.9523\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5418 - accuracy: 0.9523\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.9523\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.9523\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4299 - accuracy: 0.9523\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4004 - accuracy: 0.9523\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3740 - accuracy: 0.9523\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3507 - accuracy: 0.9523\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.9523\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.9523\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2954 - accuracy: 0.9523\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2807 - accuracy: 0.9523\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2681 - accuracy: 0.9523\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2572 - accuracy: 0.9523\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2471 - accuracy: 0.9523\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2380 - accuracy: 0.9523\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2301 - accuracy: 0.9523\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9497\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2166 - accuracy: 0.9523\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2108 - accuracy: 0.9523\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9523\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2008 - accuracy: 0.9523\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1963 - accuracy: 0.9523\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9523\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9523\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9497\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1820 - accuracy: 0.9497\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.9523\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.9497\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9497\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1716 - accuracy: 0.9497\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1694 - accuracy: 0.9497\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9497\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1654 - accuracy: 0.9497\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1637 - accuracy: 0.9523\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9523\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1604 - accuracy: 0.9497\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9523\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9523\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9523\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9523\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9523\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9523\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9523\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9523\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9523\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9497\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 0.9523\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1465 - accuracy: 0.9523\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0bbefdb00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.fit(xtrain, ytrain, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 단어 번역 함수\n",
    "def translate(word):\n",
    "    index = dialect_to_index[word]\n",
    "    one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    res = index_to_standard[pred_index]\n",
    "\n",
    "\n",
    "    # for i in range(len(sentence)):\n",
    "    #     if exist_in_dialect(sentence[i]) == True:\n",
    "    #         print(dialect_to_index[sentence[i]])\n",
    "    #         index = dialect_to_index[sentence[i]]\n",
    "    #         one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    #         pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    #         res = res + \" \" + index_to_standard[pred_index]\n",
    "\n",
    "    #     else:\n",
    "    #         res = res + \" \" + sentence[i]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-59e7f37e0a51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"자가격리 이주 했다 아이가\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-930317ba5871>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 사투리 단어 번역 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexist_in_dialect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdialect_to_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "translate(\"자가격리 이주 했다 아이가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exist_in_dialect(\"인덱스\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eojeol(standard_to_index, dialect_to_index):\n",
    "    eojeol_list=[]\n",
    "    st_list = []\n",
    "    di_list = []\n",
    "    \n",
    "    for k in standard_to_index:\n",
    "        st_list.append(k)\n",
    "        \n",
    "    for v in dialect_to_index:\n",
    "        di_list.append(v)\n",
    "        \n",
    "    for i, j in zip(st_list, di_list):\n",
    "        \n",
    "        eojeol_list.append(i)\n",
    "        \n",
    "        if i == j:\n",
    "            eojeol_list.append(j + \"0\")\n",
    "        else:\n",
    "            eojeol_list.append(j)   \n",
    "            \n",
    "    eojeol_df = pd.DataFrame(eojeol_list, columns=None)\n",
    "    eojeol_dict = eojeol_df.to_dict()\n",
    "    eojeol_dict = eojeol_dict[0]\n",
    "    eojeol_dict = {v:k for k,v in eojeol_dict.items()}\n",
    "    print(eojeol_dict)\n",
    "\n",
    "    return eojeol_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eojeol_dict = make_eojeol()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eojeol_dependence():\n",
    "    eojeol_d=[]\n",
    "    pos_d=[]\n",
    "    #dic=[]\n",
    "    re_eojeol_dict = {v:k for k,v in eojeol_dict.items()} #{인덱스:문자}\n",
    "        \n",
    "    for i in range(len(dep)): #dep(딕셔너리)길이만큼 반복 23\n",
    "        if dep[i]['text'] in eojeol_dict:\n",
    "            eojeol_d.append(eojeol_dict.values())\n",
    "        for j in range(len(dep[i]['mod'])):\n",
    "            mod=dep[i]['mod']\n",
    "            if len(mod[j]) > 0:\n",
    "                pos=re_eojeol_dict.get(mod[j])#결과값:문자\n",
    "                #mod가 가리키는 어절의 품사를 확인하는 방법..\n",
    "                if dep[i]['label'] in pos_dict:\n",
    "                    pos_d.append(pos_dict.values())\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "    result = pd.DataFrame(data = {\"word_dic\":eojeol_d, \"mod_dic\":pos_d})\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_synonym(sentence):\n",
    "    res = \"\"\n",
    "    sentence_sep = sentence.split()\n",
    "    print(sentence_sep)\n",
    "    for i in range(len(sentence_sep)):\n",
    "        print(i)\n",
    "        if exist_in_dialect(sentence_sep[i]) == True: #사투리 사전에 있으면\n",
    "            if sentence_sep[i] in eojeol_dict.keys(): #해당 단어가 전체 사전에 있으면\n",
    "\n",
    "                pos, _ = get_mods(sentence, i) #연관된 단어의 품사 받아옴\n",
    "                pos_idx = pos_dict[pos] #품사의 인덱스\n",
    "                word = sentence_sep[i] \n",
    "                word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                word_idx = eojeol_dict[word] #단어의 인덱스\n",
    "\n",
    "                #one_hot = np_utils.to_categorical(pos_idx,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                percentage = tmp_model.predict(np.array([one_hot])) # 동의어 처리 모델\n",
    "                res_idx = max(percentage[word0_idx], percentage[word_idx]) # 둘 중 더 큰 확률값\n",
    "                \n",
    "                one_hot = np_utils.to_categorical(res_idx,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                pred_index = np.argmax(translate_model.predict(np.array([one_hot]))) # 번역 모델                \n",
    "\n",
    "                print(\"번역완료\")\n",
    "\n",
    "                res = res + \" \" + index_to_standard[pred_index]\n",
    "            else: #해당 단어가 전체 사전에 없으면 -> 동의어가 없으므로 바로 번역 모델에 넣음\n",
    "                word = sentence_sep[i] #단어의 인덱스\n",
    "                word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "        else:\n",
    "            res = res + \" \" + index_to_standard[pred_index]\n",
    "            \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mods(sentence, num):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "\n",
    "    accessKey = \"aba3a45d-3318-4061-8a5d-799c8521b082\"\n",
    "    analysisCode = \"dparse\"\n",
    "\n",
    "    text = sentence\n",
    "    \n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    tmp = ast.literal_eval(response.data.decode('utf-8'))\n",
    "    dict_with_mods = tmp[\"return_object\"][\"sentence\"][0][\"dependency\"]\n",
    "\n",
    "    #print(dict_with_mods[num][\"mod\"][0])\n",
    "    id=int(dict_with_mods[num][\"mod\"][0])\n",
    "    pos = dict_with_mods[id][\"label\"]\n",
    "    \n",
    "    return pos, dict_with_mods[id][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab2d543e6fcd2c9d1adcb8a11712178d53957602a005263fff7c4bba6d50274d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('name': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
