{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"[라벨]경상도_학습데이터_1/DKCI20000001.json\", encoding=\"UTF-8\")\n",
    "raw_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리, 표준어 사전 생성 함수\n",
    "def make_dict(raw_data):\n",
    "\n",
    "    dialect_word = []\n",
    "    standard_word = [] \n",
    "    \n",
    "    \n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        eojeol = raw_data[\"utterance\"][i][\"eojeolList\"]\n",
    "            \n",
    "        for k in range(len(eojeol)):\n",
    "            if eojeol[k][\"isDialect\"] == True:\n",
    "                if k not in dialect_word:\n",
    "                    dialect_word.append(eojeol[k]['eojeol'])\n",
    "                    standard_word.append(eojeol[k]['standard'])\n",
    "\n",
    "    word_dictionary = pd.DataFrame([dialect_word, standard_word])\n",
    "    word_dictionary = word_dictionary.transpose()\n",
    "    word_dictionary.columns = ['dialect', 'standard']\n",
    "    dialect_dictionary = word_dictionary['dialect']\n",
    "    standard_dictionary = word_dictionary['standard']\n",
    "    dialect_dict = dialect_dictionary.to_dict()\n",
    "    standard_dict = standard_dictionary.to_dict()\n",
    "\n",
    "    index_to_dialect = {v:k for k,v in dialect_dict.items()}\n",
    "    index_to_standard = {v:k for k,v in standard_dict.items()}\n",
    "\n",
    "    dialect_to_index = {v:k for k,v in index_to_dialect.items()}\n",
    "    standard_to_index = {v:k for k,v in index_to_standard.items()}\n",
    "\n",
    "    print(len(index_to_dialect))\n",
    "    print(len(dialect_to_index))\n",
    "\n",
    "    print(len(index_to_standard))\n",
    "    print(len(standard_to_index))\n",
    "    \n",
    "\n",
    "    return dialect_to_index,standard_to_index,index_to_dialect,index_to_standard\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "236\n",
      "223\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "index_to_dialect,index_to_standard,dialect_to_index,standard_to_index = make_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 표준어 쌍 생성 함수\n",
    "def make_pair(raw_data):\n",
    "    filter = re.compile('[^가-힣+]')\n",
    "\n",
    "    dialect_form = []\n",
    "    standard_form = []\n",
    "\n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        sentence = raw_data[\"utterance\"][i]\n",
    "        \n",
    "        for j in range(len(sentence[\"eojeolList\"])):\n",
    "            #print(sentence[\"eojeolList\"][0]['eojeol'])\n",
    "            if sentence[\"eojeolList\"][j]['isDialect'] == True:\n",
    "                try : \n",
    "\n",
    "                    dialect = sentence[\"eojeolList\"][j][\"eojeol\"]\n",
    "                    standard = sentence[\"eojeolList\"][j][\"standard\"]\n",
    "                    \n",
    "                    dialect = filter.sub('', dialect)\n",
    "                    standard = filter.sub('', standard)\n",
    "                    \n",
    "                    dialect = dialect_to_index[dialect]\n",
    "                    standard = standard_to_index[standard]\n",
    "\n",
    "                    dialect_form.append(dialect)\n",
    "                    standard_form.append(standard)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    result = pd.DataFrame(data = {\"dialect_words\":dialect_form, \"standard_words\": standard_form})\n",
    "\n",
    "\n",
    "    return result\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = make_pair(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 사전에 존재하는지 확인하는 함수\n",
    "\n",
    "def exist_in_dialect(x):\n",
    "\n",
    "    if x in dialect_to_index:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exist_in_dialect(\"자가격리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = np_utils.to_categorical(pairs[\"dialect_words\"])\n",
    "ytrain = np_utils.to_categorical(pairs[\"standard_words\"])\n",
    "len(xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 416)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model = Sequential()\n",
    "translate_model.add(Dense(256, input_dim = 416, activation=\"relu\"))\n",
    "translate_model.add(Dense(416, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 6.0181 - accuracy: 0.0377\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.9630 - accuracy: 0.2588\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 5.9087 - accuracy: 0.3844\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.8511 - accuracy: 0.4724\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.7877 - accuracy: 0.4899\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.7174 - accuracy: 0.5176\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 5.6391 - accuracy: 0.5025\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 5.5511 - accuracy: 0.4950\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.4523 - accuracy: 0.4950\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.3420 - accuracy: 0.4824\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.2182 - accuracy: 0.4774\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.0830 - accuracy: 0.4598\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 4.9351 - accuracy: 0.4523\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 4.7702 - accuracy: 0.4347\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 4.5973 - accuracy: 0.4196\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 4.4125 - accuracy: 0.4045\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.2166 - accuracy: 0.3844\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 4.0148 - accuracy: 0.3819\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.8138 - accuracy: 0.3744\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.6122 - accuracy: 0.3693\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.4168 - accuracy: 0.3668\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.2352 - accuracy: 0.3668\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0674 - accuracy: 0.3668\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9142 - accuracy: 0.3744\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.7768 - accuracy: 0.3744\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.6524 - accuracy: 0.4045\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5327 - accuracy: 0.4372\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.4211 - accuracy: 0.4598\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.3139 - accuracy: 0.4774\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.2108 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.1090 - accuracy: 0.5075\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.0146 - accuracy: 0.5377\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9194 - accuracy: 0.5829\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.8264 - accuracy: 0.6633\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.7366 - accuracy: 0.7688\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.6478 - accuracy: 0.8518\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.5606 - accuracy: 0.9045\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4746 - accuracy: 0.9347\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3923 - accuracy: 0.9497\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.3114 - accuracy: 0.9523\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.2321 - accuracy: 0.9523\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.1544 - accuracy: 0.9523\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0779 - accuracy: 0.9523\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.0062 - accuracy: 0.9523\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9369 - accuracy: 0.9523\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8714 - accuracy: 0.9523\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8055 - accuracy: 0.9523\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7463 - accuracy: 0.9523\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.9523\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6353 - accuracy: 0.9523\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5859 - accuracy: 0.9523\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.5418 - accuracy: 0.9523\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5012 - accuracy: 0.9523\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.9523\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4299 - accuracy: 0.9523\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4004 - accuracy: 0.9523\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3740 - accuracy: 0.9523\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3507 - accuracy: 0.9523\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.9523\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.9523\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2954 - accuracy: 0.9523\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2807 - accuracy: 0.9523\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2681 - accuracy: 0.9523\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2572 - accuracy: 0.9523\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2471 - accuracy: 0.9523\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2380 - accuracy: 0.9523\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2301 - accuracy: 0.9523\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9497\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2166 - accuracy: 0.9523\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2108 - accuracy: 0.9523\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9523\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2008 - accuracy: 0.9523\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1963 - accuracy: 0.9523\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9523\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9523\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9497\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1820 - accuracy: 0.9497\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.9523\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.9497\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9497\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1716 - accuracy: 0.9497\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1694 - accuracy: 0.9497\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9497\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.1654 - accuracy: 0.9497\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1637 - accuracy: 0.9523\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9523\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1604 - accuracy: 0.9497\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9523\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9523\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9523\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9523\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.1535 - accuracy: 0.9523\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9523\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9523\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9523\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9523\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9497\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 0.9523\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1465 - accuracy: 0.9523\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0bbefdb00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.fit(xtrain, ytrain, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 단어 번역 함수\n",
    "def translate(word):\n",
    "    index = dialect_to_index[word]\n",
    "    one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    res = index_to_standard[pred_index]\n",
    "\n",
    "\n",
    "    # for i in range(len(sentence)):\n",
    "    #     if exist_in_dialect(sentence[i]) == True:\n",
    "    #         print(dialect_to_index[sentence[i]])\n",
    "    #         index = dialect_to_index[sentence[i]]\n",
    "    #         one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    #         pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    #         res = res + \" \" + index_to_standard[pred_index]\n",
    "\n",
    "    #     else:\n",
    "    #         res = res + \" \" + sentence[i]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\"NNG\": 0, \"NNP\": 1, \"NNB\": 2, \"NP\": 3, \"NR\": 4, \"VV\": 5,\n",
    "           \"VA\": 6, \"VX\": 7, \"VCP\": 8, \"VCN\": 9, \"MM\": 10, \"MAG\": 11,\n",
    "           \"MAJ\": 12, \"IC\": 13, \"JKS\": 14, \"JKC\": 15, \"JKG\": 16, \"JKO\": 17,\n",
    "           \"JKB\": 18, \"JKV\": 19, \"JKQ\": 20, \"JX\": 21, \"JC\": 22, \"EP\": 23,\n",
    "           \"EF\": 24, \"EC\": 25, \"ETN\": 26, \"ETM\": 27, \"XPN\": 28, \"XSN\": 29,\n",
    "           \"XSV\": 30, \"XSA\": 31, \"XR\": 32, \"SF\":  33, \"SP\": 34, \"SS\": 35,\n",
    "           \"SE\": 36, \"SO\": 37,  \"SL\": 38, \"SH\": 39, \"SW\": 40, \"NF\": 41,\n",
    "           \"NV\": 42, \"SN\": 43, \"NA\": 44}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 어절 사전 생성 함수\n",
    "def make_eojeol(standard_to_index, dialect_to_index):\n",
    "    eojeol_list=[]\n",
    "    st_list = []\n",
    "    di_list = []\n",
    "    \n",
    "    for k in standard_to_index:\n",
    "        st_list.append(k)\n",
    "        \n",
    "    for v in dialect_to_index:\n",
    "        di_list.append(v)\n",
    "        \n",
    "    for i, j in zip(st_list, di_list):\n",
    "        \n",
    "        eojeol_list.append(i)\n",
    "        \n",
    "        if i == j:\n",
    "            eojeol_list.append(j + \"0\")\n",
    "        else:\n",
    "            eojeol_list.append(j)   \n",
    "            \n",
    "    eojeol_df = pd.DataFrame(eojeol_list, columns=None)\n",
    "    eojeol_dict = eojeol_df.to_dict()\n",
    "    eojeol_dict = eojeol_dict[0]\n",
    "    eojeol_dict = {v:k for k,v in eojeol_dict.items()}\n",
    "    #print(eojeol_dict)\n",
    "\n",
    "    return eojeol_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'먹': 0,\n",
       " '묵': 1,\n",
       " '그리고': 2,\n",
       " '그라고': 3,\n",
       " '아니야.': 4,\n",
       " '아이가.': 5,\n",
       " '무엇이냐면': 6,\n",
       " '머냐면': 7,\n",
       " '인제': 21,\n",
       " '인자': 9,\n",
       " '가서': 10,\n",
       " '가가지고': 11,\n",
       " '먹지.': 12,\n",
       " '묵지.': 13,\n",
       " '아무거나': 14,\n",
       " '아무꺼나': 15,\n",
       " '조금': 16,\n",
       " '쪼끔': 17,\n",
       " '많더라': 18,\n",
       " '많드라': 19,\n",
       " '이제': 20,\n",
       " '바뀐다며': 22,\n",
       " '바뀐다매': 23,\n",
       " '이말이지': 24,\n",
       " '이말인거라': 25,\n",
       " '그러니까': 26,\n",
       " '그러니깐': 27,\n",
       " '먹었거든.': 28,\n",
       " '먹겄든.': 29,\n",
       " '컸고': 30,\n",
       " '큰기고': 31,\n",
       " '먹어져': 32,\n",
       " '먹어지데': 33,\n",
       " '왜': 34,\n",
       " '와': 35,\n",
       " '무엇이라': 36,\n",
       " '머라': 37,\n",
       " '엄청': 38,\n",
       " '억수로': 39,\n",
       " '길지': 40,\n",
       " '길다': 41,\n",
       " '않니': 42,\n",
       " '아이가': 43,\n",
       " '않습니까': 44,\n",
       " '아입니까': 45,\n",
       " '그러면': 46,\n",
       " '그라면은': 47,\n",
       " '무조건': 48,\n",
       " '그니까': 49,\n",
       " '길다고했다': 50,\n",
       " '무조끈': 51,\n",
       " '나를': 52,\n",
       " '길다캐따': 53,\n",
       " '좋더라.': 54,\n",
       " '내를': 55,\n",
       " '일단': 56,\n",
       " '좋는기라.': 57,\n",
       " '먹고': 58,\n",
       " '일딴': 59,\n",
       " '쓰리지': 60,\n",
       " '묵고': 61,\n",
       " '무엇을': 62,\n",
       " '쓰리다': 63,\n",
       " '먹고해야지': 64,\n",
       " '아닙니까': 65,\n",
       " '먹은지': 66,\n",
       " '멀': 67,\n",
       " '나': 68,\n",
       " '묵고해야지': 69,\n",
       " '맞지': 70,\n",
       " '묵은지가': 71,\n",
       " '않니.': 72,\n",
       " '내': 73,\n",
       " '가지고': 74,\n",
       " '맞다': 75,\n",
       " '먹었지': 76,\n",
       " '가꼬': 77,\n",
       " '이래서': 78,\n",
       " '먹었다': 79,\n",
       " '그렇게해야': 80,\n",
       " '이래가': 81,\n",
       " '그렇게해서': 82,\n",
       " '쫌': 83,\n",
       " '나만': 84,\n",
       " '그래해야': 85,\n",
       " '되네?': 86,\n",
       " '그래가': 87,\n",
       " '먹으니': 88,\n",
       " '내만': 89,\n",
       " '먹으면': 90,\n",
       " '되뿌네?': 91,\n",
       " '하겠나': 92,\n",
       " '묵으니': 93,\n",
       " '그렇고': 94,\n",
       " '무그면': 95,\n",
       " '된다.': 96,\n",
       " '한다드노': 97,\n",
       " '그렇지': 98,\n",
       " '고로코': 99,\n",
       " '나나': 100,\n",
       " '되는기라.': 101,\n",
       " '시체이지': 102,\n",
       " '그쟈': 103,\n",
       " '했지': 104,\n",
       " '내나': 105,\n",
       " '해먹어야될': 106,\n",
       " '쪼금': 107,\n",
       " '가르쳐주더라': 108,\n",
       " '시체다': 109,\n",
       " '해먹으면': 110,\n",
       " '캐닷고': 111,\n",
       " '구워서': 112,\n",
       " '해무야될': 113,\n",
       " '굽으니깐': 114,\n",
       " '가르쳐주드라고': 115,\n",
       " '나더라고': 116,\n",
       " '해무면은': 117,\n",
       " '뒤집습니다.': 118,\n",
       " '꾸가': 119,\n",
       " '뒤집어서': 120,\n",
       " '꾸브니깐': 121,\n",
       " '오므립니다': 122,\n",
       " '나오드라고': 123,\n",
       " '못밨지': 124,\n",
       " '묵꼬': 125,\n",
       " '왜냐하면': 126,\n",
       " '뒤빕니다.': 127,\n",
       " '굽는거': 128,\n",
       " '뒤벼서': 129,\n",
       " '뭐라고하니': 130,\n",
       " '오므릅니다': 131,\n",
       " '고소한데': 132,\n",
       " '못봤다': 133,\n",
       " '굽고': 134,\n",
       " '왜냐면': 135,\n",
       " '뒤집어가지고': 136,\n",
       " '꿉는거': 137,\n",
       " '굽어가지고': 138,\n",
       " '머라카노': 139,\n",
       " '되더라고': 140,\n",
       " '꼬신데': 141,\n",
       " '그것': 142,\n",
       " '꿉고': 143,\n",
       " '한다': 283,\n",
       " '뒤비가지고': 145,\n",
       " '그것까지': 146,\n",
       " '꾸어가지고': 147,\n",
       " '한게': 148,\n",
       " '되드라고': 149,\n",
       " '그것이고': 150,\n",
       " '고거': 151,\n",
       " '거기다가': 152,\n",
       " '하드라고': 153,\n",
       " '고추가루': 154,\n",
       " '고거까지': 155,\n",
       " '고추가루를': 156,\n",
       " '핸게': 157,\n",
       " '없어서': 158,\n",
       " '고거고': 159,\n",
       " '그것가지고': 160,\n",
       " '머냐면은': 161,\n",
       " '변하더라고': 162,\n",
       " '그다가': 163,\n",
       " '하더라고': 164,\n",
       " '꼬추가루': 165,\n",
       " '그곳에서': 166,\n",
       " '꼬추가루를': 167,\n",
       " '그렇게': 168,\n",
       " '없어가': 169,\n",
       " '해먹었는데': 170,\n",
       " '그거가따가': 171,\n",
       " '뭐라하지': 172,\n",
       " '변하드라고': 173,\n",
       " '나는거야': 174,\n",
       " '거서': 175,\n",
       " '해먹야되냐면': 176,\n",
       " '고롷게': 177,\n",
       " '먹어야하는데': 178,\n",
       " '해뭇는데': 179,\n",
       " '그것도': 180,\n",
       " '머라노': 181,\n",
       " '약간하고': 182,\n",
       " '나는기야': 183,\n",
       " '넣고': 184,\n",
       " '해무야되냐면': 185,\n",
       " '들여서': 186,\n",
       " '무야하는데': 187,\n",
       " '들어내서': 188,\n",
       " '고것도': 189,\n",
       " '해서 먹고': 190,\n",
       " '약간치고': 191,\n",
       " '먹을': 192,\n",
       " '옇고': 193,\n",
       " '해먹을만': 194,\n",
       " '들여가': 195,\n",
       " '그것하고': 196,\n",
       " '들어내가': 197,\n",
       " '그러면서': 198,\n",
       " '해묵고': 199,\n",
       " '졸이더라고': 200,\n",
       " '물': 201,\n",
       " '그것이': 202,\n",
       " '해묵을만': 203,\n",
       " '뭐야': 204,\n",
       " '고거하고': 205,\n",
       " '뭐라고하지': 206,\n",
       " '넣드라고': 207,\n",
       " '먹을란다': 208,\n",
       " '그라면서': 209,\n",
       " '무엇인지': 210,\n",
       " '쫄이드라고': 211,\n",
       " '먹어도': 212,\n",
       " '그기': 213,\n",
       " '먹을만하단거지': 214,\n",
       " '뭐고': 215,\n",
       " '먹으니깐': 216,\n",
       " '물란다': 217,\n",
       " '그것은': 218,\n",
       " '머고': 219,\n",
       " '주는거라': 220,\n",
       " '무도': 221,\n",
       " '이것도': 222,\n",
       " '물만하단거지': 223,\n",
       " '저것도': 224,\n",
       " '무그니깐': 225,\n",
       " '뭐지': 226,\n",
       " '고는': 227,\n",
       " '힘들지?': 228,\n",
       " '주는기라': 229,\n",
       " '사서먹어야되는': 230,\n",
       " '요고도': 231,\n",
       " '굽는법': 232,\n",
       " '조고도': 233,\n",
       " '두꺼워가지고': 234,\n",
       " '힘들제?': 235,\n",
       " '굽어서': 236,\n",
       " '그리': 237,\n",
       " '구워도': 238,\n",
       " '사무야되는': 239,\n",
       " '구운': 240,\n",
       " '꿉는법': 241,\n",
       " '구운걸': 242,\n",
       " '뚜끄버가지고': 243,\n",
       " '끓여': 244,\n",
       " '꿉어서': 245,\n",
       " '굽으면': 246,\n",
       " '꾸워도': 247,\n",
       " '굽으면서': 248,\n",
       " '꾸운': 249,\n",
       " '잘라서': 250,\n",
       " '꾸운걸': 251,\n",
       " '구워가지고': 252,\n",
       " '끼리': 253,\n",
       " '숯불인거야.': 254,\n",
       " '꾸우면': 255,\n",
       " '맛있지': 256,\n",
       " '꾸우면서': 257,\n",
       " '진짜': 258,\n",
       " '짤라가': 259,\n",
       " '튀긴것': 260,\n",
       " '그래서 그리': 261,\n",
       " '먹어': 262,\n",
       " '꾸워가지고': 263,\n",
       " '많이': 264,\n",
       " '숯불인기라.': 265,\n",
       " '먹어서': 266,\n",
       " '맛있다': 416,\n",
       " '올려서': 268,\n",
       " '찐짜': 269,\n",
       " '올려': 270,\n",
       " '튀기난거': 271,\n",
       " '하지': 272,\n",
       " '무': 273,\n",
       " '과열하지': 274,\n",
       " '마니': 275,\n",
       " '아니야': 276,\n",
       " '무서': 277,\n",
       " '이번에': 278,\n",
       " '올리가': 279,\n",
       " '샀나?': 280,\n",
       " '올리': 281,\n",
       " '그것을': 282,\n",
       " '했거든': 284,\n",
       " '과열한다': 285,\n",
       " '어찌하는지': 286,\n",
       " '그니깐': 287,\n",
       " '찾아보니깐': 288,\n",
       " '요번에': 289,\n",
       " '이거를': 290,\n",
       " '샀노?': 291,\n",
       " '얘들이': 292,\n",
       " '그거를': 293,\n",
       " '하더라': 294,\n",
       " '했걸랑': 295,\n",
       " '얘를': 296,\n",
       " '어찌하는고': 297,\n",
       " '꺼냈어': 298,\n",
       " '찾아본끼닝': 299,\n",
       " '얘가': 300,\n",
       " '아를': 301,\n",
       " '쳤다대': 302,\n",
       " '가따가': 303,\n",
       " '난뒤에': 304,\n",
       " '아들이': 305,\n",
       " '죽겠더라고': 306,\n",
       " '하데': 307,\n",
       " '사서 와서': 308,\n",
       " '꺼냇는기라': 309,\n",
       " '그래서': 310,\n",
       " '아가': 311,\n",
       " '얼리면': 312,\n",
       " '칫다카대': 313,\n",
       " '넣어났어': 314,\n",
       " '나니까네': 315,\n",
       " '자기': 316,\n",
       " '죽겠드라고': 317,\n",
       " '담궈': 318,\n",
       " '사가와써': 319,\n",
       " '놓는거라': 320,\n",
       " '그라': 321,\n",
       " '보러': 322,\n",
       " '어루면': 323,\n",
       " '와라': 324,\n",
       " '넣어놨데': 325,\n",
       " '왔어': 326,\n",
       " '지': 327,\n",
       " '잡고': 328,\n",
       " '담가': 329,\n",
       " '나서': 330,\n",
       " '노는기라': 331,\n",
       " '있잖아': 332,\n",
       " '보로': 333,\n",
       " '화가나': 334,\n",
       " '온나': 335,\n",
       " '있어': 336,\n",
       " '왔는기라': 337,\n",
       " '이렇게': 338,\n",
       " '잡꼬': 339,\n",
       " '이렇게한거야': 340,\n",
       " '나가': 341,\n",
       " '이만한': 342,\n",
       " '아닛나': 343,\n",
       " '잡을려고': 344,\n",
       " '성이나': 345,\n",
       " '이러는거야': 346,\n",
       " '아있나': 347,\n",
       " '장갑껴야': 348,\n",
       " '아잇나': 349,\n",
       " '물었어': 350,\n",
       " '요래': 351,\n",
       " '물어버렸어': 352,\n",
       " '요러한기래': 353,\n",
       " '집었는데': 354,\n",
       " '이따만': 355,\n",
       " '살아서': 356,\n",
       " '집을라고': 357,\n",
       " '해놓으니까': 358,\n",
       " '이러는기라': 359,\n",
       " '무서워하니까': 360,\n",
       " '장갑끼야': 361,\n",
       " '도와주더라': 362,\n",
       " '물어삐써': 363,\n",
       " '시켰지': 364,\n",
       " '물어뿐기라': 365,\n",
       " '사서가져온거': 366,\n",
       " '찝었는데': 367,\n",
       " '그다음에': 368,\n",
       " '살아간': 369,\n",
       " '줬지': 370,\n",
       " '맛있드라': 371,\n",
       " '시장보고': 372,\n",
       " '해난께': 373,\n",
       " '몰라서': 374,\n",
       " '무서워한께': 375,\n",
       " '갈랐다며?': 376,\n",
       " '도와주데': 377,\n",
       " '흔했지': 378,\n",
       " '시킷다': 379,\n",
       " '모르지?': 380,\n",
       " '사가온거': 381,\n",
       " '지금': 382,\n",
       " '그담에': 383,\n",
       " '세일했지': 384,\n",
       " '주드라': 385,\n",
       " '하니까': 386,\n",
       " '시장바가': 387,\n",
       " '모르는거': 388,\n",
       " '몰라가': 389,\n",
       " '사와': 390,\n",
       " '쨋다매?': 391,\n",
       " '안되지': 392,\n",
       " '흔했다': 393,\n",
       " '어쨋든': 394,\n",
       " '모르제?': 395,\n",
       " '이번에는': 396,\n",
       " '찌끔': 397,\n",
       " '짤랐는데': 398,\n",
       " '세일했다': 399,\n",
       " '쑤셔넣으니': 400,\n",
       " '하니까네': 401,\n",
       " '났지': 402,\n",
       " '모르는기': 403,\n",
       " '많다': 404,\n",
       " '사가와': 405,\n",
       " '너': 406,\n",
       " '했다': 407,\n",
       " '기어나오고': 408,\n",
       " '안되되': 409,\n",
       " '그랬지': 410,\n",
       " '우째든': 411,\n",
       " '요즘에는': 412,\n",
       " '꺼내데': 413,\n",
       " '구워먹으니': 414,\n",
       " '요번에는': 415,\n",
       " '짤라가께': 417,\n",
       " '없었거든': 418,\n",
       " '쑤시난게': 419,\n",
       " '좋은건가': 420,\n",
       " '났다': 421,\n",
       " '아니야?': 422,\n",
       " '많데이': 423,\n",
       " '장만해서': 424,\n",
       " '니': 425,\n",
       " '꺼내먹으면': 426,\n",
       " '기나오고': 427,\n",
       " '구워먹을려고': 428,\n",
       " '그랬다': 429,\n",
       " '사가져오면': 430,\n",
       " '요새는': 431,\n",
       " '그거': 432,\n",
       " '무니깐': 433,\n",
       " '장어': 434,\n",
       " '꾸문게': 435,\n",
       " '장아찌하듯이': 436,\n",
       " '맛있데': 437,\n",
       " '잘못하면': 438,\n",
       " '꾸무께나': 439,\n",
       " '남이 하는거': 440,\n",
       " '없드라고': 441,\n",
       " '무리더라.': 442,\n",
       " '좋은긴가': 443,\n",
       " '이거': 444,\n",
       " '아니가?': 445}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eojeol_dict = make_eojeol(standard_to_index, dialect_to_index)\n",
    "eojeol_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 인덱스와 연결된 단어의 품사 인덱스\n",
    "def eojeol_dependence():\n",
    "\n",
    "    # 언어 분석 기술(문어)\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    accessKey = \"aba3a45d-3318-4061-8a5d-799c8521b082\"\n",
    "    analysisCode = \"dparse\"\n",
    "\n",
    "    text = \"엑소브레인은 내 몸 바깥에 있는 인공 두뇌라는 뜻으로, 세계 최고인공지능 기술 선도라는 비전을 달성하기 위한 과학기술정보통신부 소프트웨어 분야의 국가 혁신기술 개발형 연구개발 과제이다.\"\n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    s=ast.literal_eval(response.data.decode('utf-8'))\n",
    "\n",
    "    dep=s['return_object']['sentence'][0]['dependency']\n",
    "    \n",
    "    eojeol_d=[]\n",
    "    pos_d=[]\n",
    "    #dic=[]\n",
    "    re_eojeol_dict = {v:k for k,v in eojeol_dict.items()} #{인덱스:문자}\n",
    "        \n",
    "    for i in range(len(dep)): #dep(딕셔너리)길이만큼 반복 23\n",
    "        if dep[i]['text'] in eojeol_dict:\n",
    "            eojeol_d.append(eojeol_dict.values())\n",
    "        for j in range(len(dep[i]['mod'])):\n",
    "            mod=dep[i]['mod']\n",
    "            print(mod[j])\n",
    "            if len(mod[j]) > 0:\n",
    "                pos=re_eojeol_dict.get(mod[j])#결과값:문자\n",
    "                #mod가 가리키는 어절의 품사를 확인하는 방법..\n",
    "                if dep[i]['label'] in pos_dict:\n",
    "                    pos_d.append(pos_dict.values())\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "    result = pd.DataFrame(data = {\"word_dic\":eojeol_d, \"mod_dic\":pos_d})\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-44358910186a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meojeol_dependence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-3b92860d256f>\u001b[0m in \u001b[0;36meojeol_dependence\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mmod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mod'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre_eojeol_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#결과값:문자\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m#mod가 가리키는 어절의 품사를 확인하는 방법..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "eojeol_dependence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동의어 처리를 포함한 사투리 번역 함수\n",
    "def predict_synonym(sentence):\n",
    "    res = \"\"\n",
    "    sentence_sep = sentence.split()\n",
    "    print(sentence_sep)\n",
    "    for i in range(len(sentence_sep)):\n",
    "        print(i)\n",
    "        if exist_in_dialect(sentence_sep[i]) == True: #사투리 사전에 있으면\n",
    "            if sentence_sep[i] in eojeol_dict.keys(): #해당 단어가 전체 사전에 있으면\n",
    "\n",
    "                pos, _ = get_mods(sentence, i) #연관된 단어의 품사 받아옴\n",
    "                pos_idx = pos_dict[pos] #품사의 인덱스\n",
    "                word = sentence_sep[i] \n",
    "                word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                word_idx = eojeol_dict[word] #단어의 인덱스\n",
    "\n",
    "                #one_hot = np_utils.to_categorical(pos_idx,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                percentage = tmp_model.predict(np.array([one_hot])) # 동의어 처리 모델\n",
    "                res_idx = max(percentage[word0_idx], percentage[word_idx]) # 둘 중 더 큰 확률값\n",
    "                \n",
    "                one_hot = np_utils.to_categorical(res_idx,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                pred_index = np.argmax(translate_model.predict(np.array([one_hot]))) # 번역 모델                \n",
    "\n",
    "                print(\"번역완료\")\n",
    "\n",
    "                res = res + \" \" + index_to_standard[pred_index]\n",
    "            else: #해당 단어가 전체 사전에 없으면 -> 동의어가 없으므로 바로 번역 모델에 넣음\n",
    "                word = sentence_sep[i] #단어의 인덱스\n",
    "                word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "        else:\n",
    "            res = res + \" \" + index_to_standard[pred_index]\n",
    "            \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 연관된 단어의 품사 추출\n",
    "def get_mods(sentence, num):\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "\n",
    "    accessKey = \"aba3a45d-3318-4061-8a5d-799c8521b082\"\n",
    "    analysisCode = \"dparse\"\n",
    "\n",
    "    text = sentence\n",
    "    \n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    tmp = ast.literal_eval(response.data.decode('utf-8'))\n",
    "    dict_with_mods = tmp[\"return_object\"][\"sentence\"][0][\"dependency\"]\n",
    "\n",
    "    #print(dict_with_mods[num][\"mod\"][0])\n",
    "    id=int(dict_with_mods[num][\"mod\"][0])\n",
    "    pos = dict_with_mods[id][\"label\"]\n",
    "    \n",
    "    return pos, dict_with_mods[id][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab2d543e6fcd2c9d1adcb8a11712178d53957602a005263fff7c4bba6d50274d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('name': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
