{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/AlexYoung/Downloads/proj/proj_3/[라벨]경상도_학습데이터_1'\n",
    "file_list=os.listdir(path)\n",
    "file_list_py=[file for file in file_list if file.endswith('.json')]\n",
    "file_list_py.sort()\n",
    "#print(file_list_py)\n",
    "data=file_list_py[0:100]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"[라벨]경상도_학습데이터_1/DKCI20000001.json\", encoding=\"UTF-8\")\n",
    "raw_data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data[\"utterance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 1, 2: 2, 3: 3}, {4: 4, 5: 5}, {1: 11, 2: 22, 3: 33}, {4: 44, 5: 55}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[{1:1,2:2,3:3},{4:4,5:5}]\n",
    "b=[{1:11,2:22,3:33},{4:44,5:55}]\n",
    "a.extend(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data={\"utterance\":[]}\n",
    "for k in range(0,100):\n",
    "    name=\"DKSR200000\"+str(k)+\".json\"\n",
    "    try:\n",
    "        f = open(\"[라벨]경상도_학습데이터_1/\"+name, encoding=\"UTF-8\")\n",
    "        data = json.loads(f.read())\n",
    "        raw_data[\"utterance\"].extend(data[\"utterance\"])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'DKSR20000011.1.1.281',\n",
       " 'form': '그런 데가 재밌고 나머지는 다 비슷비슷한 거 같애.',\n",
       " 'standard_form': '그런 데가 재밌고 나머지는 다 비슷비슷한 거 같애.',\n",
       " 'dialect_form': '그런 데가 재밌고 나머지는 다 비슷비슷한 거 같애.',\n",
       " 'speaker_id': 'SR0062',\n",
       " 'start': 1324.68,\n",
       " 'end': 1330.89,\n",
       " 'note': '',\n",
       " 'eojeolList': [{'id': 1,\n",
       "   'eojeol': '그런',\n",
       "   'standard': '그런',\n",
       "   'isDialect': False},\n",
       "  {'id': 2, 'eojeol': '데가', 'standard': '데가', 'isDialect': False},\n",
       "  {'id': 3, 'eojeol': '재밌고', 'standard': '재밌고', 'isDialect': False},\n",
       "  {'id': 4, 'eojeol': '나머지는', 'standard': '나머지는', 'isDialect': False},\n",
       "  {'id': 5, 'eojeol': '다', 'standard': '다', 'isDialect': False},\n",
       "  {'id': 6, 'eojeol': '비슷비슷한', 'standard': '비슷비슷한', 'isDialect': False},\n",
       "  {'id': 7, 'eojeol': '거', 'standard': '거', 'isDialect': False},\n",
       "  {'id': 8, 'eojeol': '같애.', 'standard': '같애.', 'isDialect': False}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"utterance\"][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data[\"utterance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리, 표준어 사전 생성 함수\n",
    "def make_dict(raw_data):\n",
    "\n",
    "    dialect_word = []\n",
    "    standard_word = [] \n",
    "    all_std_word=[]\n",
    "\n",
    "    filter = re.compile('[^가-힣 +]')    \n",
    "    \n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        #print(raw_data[\"utterance\"])\n",
    "        eojeol = raw_data[\"utterance\"][i][\"eojeolList\"]\n",
    "            \n",
    "        for k in range(len(eojeol)):\n",
    "            if eojeol[k][\"isDialect\"] == True:\n",
    "                if k not in dialect_word:\n",
    "                    dlt = filter.sub('', eojeol[k]['eojeol'])\n",
    "                    std = filter.sub('', eojeol[k]['standard'])\n",
    "                    dialect_word.append(dlt)\n",
    "                    standard_word.append(std)\n",
    "            else:\n",
    "                if k not in standard_word:\n",
    "                    word = filter.sub('', eojeol[k]['standard'])\n",
    "                    all_std_word.append(word)\n",
    "\n",
    "    word_dictionary = pd.DataFrame([dialect_word, standard_word])\n",
    "    word_dictionary = word_dictionary.transpose()\n",
    "    word_dictionary.columns = ['dialect', 'standard']\n",
    "    dialect_dictionary = word_dictionary['dialect']\n",
    "    standard_dictionary = word_dictionary['standard']\n",
    "    dialect_dict = dialect_dictionary.to_dict()\n",
    "    standard_dict = standard_dictionary.to_dict()\n",
    "\n",
    "    index_to_dialect = {v:k for k,v in dialect_dict.items()}\n",
    "    index_to_standard = {v:k for k,v in standard_dict.items()}\n",
    "\n",
    "    dialect_to_index = {v:k for k,v in index_to_dialect.items()}\n",
    "    standard_to_index = {v:k for k,v in index_to_standard.items()}    \n",
    "\n",
    "    return dialect_to_index,standard_to_index,index_to_dialect,index_to_standard,all_std_word\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_dialect,index_to_standard,dialect_to_index,standard_to_index, all_std_word = make_dict(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 표준어 쌍 생성 함수\n",
    "def make_pair(raw_data):\n",
    "    filter = re.compile('[^가-힣+]')\n",
    "\n",
    "    dialect_form = []\n",
    "    standard_form = []\n",
    "\n",
    "    for i in range(len(raw_data[\"utterance\"])):\n",
    "        sentence = raw_data[\"utterance\"][i]\n",
    "        \n",
    "        for j in range(len(sentence[\"eojeolList\"])):\n",
    "            #print(sentence[\"eojeolList\"][0]['eojeol'])\n",
    "            if sentence[\"eojeolList\"][j]['isDialect'] == True:\n",
    "                try : \n",
    "\n",
    "                    dialect = sentence[\"eojeolList\"][j][\"eojeol\"]\n",
    "                    standard = sentence[\"eojeolList\"][j][\"standard\"]\n",
    "                    \n",
    "                    dialect = filter.sub('', dialect)\n",
    "                    standard = filter.sub('', standard)\n",
    "                    \n",
    "                    dialect = dialect_to_index[dialect]\n",
    "                    standard = standard_to_index[standard]\n",
    "\n",
    "                    dialect_form.append(dialect)\n",
    "                    standard_form.append(standard)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    result = pd.DataFrame(data = {\"dialect_words\":dialect_form, \"standard_words\": standard_form})\n",
    "\n",
    "\n",
    "    return result\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialect_words</th>\n",
       "      <th>standard_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>793</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>920</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>1210</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dialect_words  standard_words\n",
       "0               920             920\n",
       "1               793             793\n",
       "2                 2             920\n",
       "3               920             920\n",
       "4              1210            1210\n",
       "...             ...             ...\n",
       "1196           1210            1210\n",
       "1197           1210            1210\n",
       "1198           1210            1210\n",
       "1199           1210            1210\n",
       "1200           1210            1210\n",
       "\n",
       "[1201 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = make_pair(raw_data)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 사전에 존재하는지 확인하는 함수\n",
    "def exist_in_dialect(x):\n",
    "\n",
    "    if x in dialect_to_index:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_xtrain = np_utils.to_categorical(pairs[\"dialect_words\"])\n",
    "trans_ytrain = np_utils.to_categorical(pairs[\"standard_words\"])\n",
    "len(trans_xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 1211)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_xtrain.shape\n",
    "trans_ytrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model = Sequential()\n",
    "translate_model.add(Dense(256, input_dim = 1211, activation=\"relu\"))\n",
    "translate_model.add(Dense(1211, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 16ms/step - loss: 7.0401 - accuracy: 0.3872\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 6.8630 - accuracy: 0.5604\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 6.5843 - accuracy: 0.5504\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 6.1412 - accuracy: 0.5154\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 5.4721 - accuracy: 0.4538\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 4.5982 - accuracy: 0.4471\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 3.7119 - accuracy: 0.4471\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 3.1191 - accuracy: 0.4471\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 2.7722 - accuracy: 0.4538\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 2.5262 - accuracy: 0.4921\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 2.3344 - accuracy: 0.5271\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 2.1640 - accuracy: 0.5754\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 2.0088 - accuracy: 0.6137\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 1.8658 - accuracy: 0.6311\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 1.7351 - accuracy: 0.6503\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 1.6124 - accuracy: 0.6694\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 1.4999 - accuracy: 0.6853\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 1.3928 - accuracy: 0.7011\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.2908 - accuracy: 0.7277\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 1.1892 - accuracy: 0.7485\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 1.0973 - accuracy: 0.7544\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 1.0062 - accuracy: 0.7577\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.9195 - accuracy: 0.7644\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.8364 - accuracy: 0.7902\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.7521 - accuracy: 0.8793\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6756 - accuracy: 0.9434\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6042 - accuracy: 0.9734\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.5341 - accuracy: 0.9825\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.4693 - accuracy: 0.9850\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.4111 - accuracy: 0.9842\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.3569 - accuracy: 0.9842\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.3092 - accuracy: 0.9850\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.2673 - accuracy: 0.9850\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.2328 - accuracy: 0.9858\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.2037 - accuracy: 0.9858\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.1808 - accuracy: 0.9842\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1618 - accuracy: 0.9850\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.1458 - accuracy: 0.9850\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.1330 - accuracy: 0.9842\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.1225 - accuracy: 0.9858\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.1139 - accuracy: 0.9850\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.1064 - accuracy: 0.9850\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.1020 - accuracy: 0.9842\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0976 - accuracy: 0.9850\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0926 - accuracy: 0.9842\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0878 - accuracy: 0.9850\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0839 - accuracy: 0.9842\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0805 - accuracy: 0.9858\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0774 - accuracy: 0.9850\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0749 - accuracy: 0.9850\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0723 - accuracy: 0.9842\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0702 - accuracy: 0.9850\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0684 - accuracy: 0.9850\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0664 - accuracy: 0.9858\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.0649 - accuracy: 0.9850\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0632 - accuracy: 0.9858\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0618 - accuracy: 0.9850\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0603 - accuracy: 0.9850\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0591 - accuracy: 0.9858\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0579 - accuracy: 0.9867\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0568 - accuracy: 0.9858\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0561 - accuracy: 0.9858\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9850\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0540 - accuracy: 0.9842\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0533 - accuracy: 0.9850\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0526 - accuracy: 0.9867\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0517 - accuracy: 0.9858\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0511 - accuracy: 0.9875\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0504 - accuracy: 0.9858\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0499 - accuracy: 0.9867\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0492 - accuracy: 0.9867\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0488 - accuracy: 0.9875\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0481 - accuracy: 0.9858\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0477 - accuracy: 0.9867\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0473 - accuracy: 0.9875\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0467 - accuracy: 0.9875\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0464 - accuracy: 0.9875\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0457 - accuracy: 0.9883\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0453 - accuracy: 0.9875\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0450 - accuracy: 0.9883\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0448 - accuracy: 0.9883\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0442 - accuracy: 0.9883\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0438 - accuracy: 0.9883\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.0436 - accuracy: 0.9875\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 0.0431 - accuracy: 0.9883\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.9883\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0426 - accuracy: 0.9883\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9883\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.0422 - accuracy: 0.9875\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0417 - accuracy: 0.9867\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0414 - accuracy: 0.9883\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.0411 - accuracy: 0.9867\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0409 - accuracy: 0.9875\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.0420 - accuracy: 0.9875\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0417 - accuracy: 0.9883\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0415 - accuracy: 0.9875\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0413 - accuracy: 0.9875\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.0409 - accuracy: 0.9883\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.0407 - accuracy: 0.9883\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 0.0405 - accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c54aeddda0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_model.fit(trans_xtrain, trans_ytrain, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 단어 번역 함수\n",
    "def translate(word):\n",
    "    index = dialect_to_index[word]\n",
    "    one_hot = np_utils.to_categorical(index,len(trans_xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    print(translate_model.predict(np.array([one_hot])))\n",
    "    pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    res = index_to_standard[pred_index]\n",
    "\n",
    "\n",
    "    # for i in range(len(sentence)):\n",
    "    #     if exist_in_dialect(sentence[i]) == True:\n",
    "    #         print(dialect_to_index[sentence[i]])\n",
    "    #         index = dialect_to_index[sentence[i]]\n",
    "    #         one_hot = np_utils.to_categorical(index,len(xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "    #         pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "    #         res = res + \" \" + index_to_standard[pred_index]\n",
    "\n",
    "    #     else:\n",
    "    #         res = res + \" \" + sentence[i]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4975409e-07 2.7200417e-07 2.4866893e-07 ... 2.2092448e-07\n",
      "  2.5114656e-07 1.4238237e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'했지'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"했다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tag = ['NP','VP','AP','VNP','DP','IP','X','L','R']\n",
    "b_tag = ['SBJ','OBJ','MOD','AJT','CMP','CNJ']\n",
    "c_tag = []\n",
    "pos_dict = {}\n",
    "for i in a_tag:\n",
    "    for j in b_tag:\n",
    "        c_tag.append(i+\"_\"+j)\n",
    "c_tag.extend(a_tag)\n",
    "c_tag.extend(b_tag)\n",
    "for i in range(len(c_tag)):\n",
    "    pos_dict[c_tag[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eojeol(standard_to_index, dialect_to_index, all_std_word):\n",
    "\n",
    "    eojeol_list=[]\n",
    "    st_list = []\n",
    "    di_list = []\n",
    "\n",
    "\n",
    "    for k in standard_to_index:\n",
    "        st_list.append(k)\n",
    "\n",
    "    for v in dialect_to_index:\n",
    "        di_list.append(v)     \n",
    "\n",
    "    for i, j in zip(st_list, di_list):\n",
    "        eojeol_list.append(i)\n",
    "        eojeol_list.append(j + \"0\")\n",
    "        \n",
    "    for i in all_std_word:\n",
    "        eojeol_list.append(i) \n",
    "\n",
    "    eojeol_df = pd.DataFrame(eojeol_list, columns=None)\n",
    "    eojeol_dict = eojeol_df.to_dict()\n",
    "    eojeol_dict = eojeol_dict[0]\n",
    "    eojeol_dict = {v:k for k,v in eojeol_dict.items()}\n",
    "\n",
    "    #print(eojeol_dict)\n",
    "\n",
    "    return eojeol_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eojeol_dict = make_eojeol(standard_to_index, dialect_to_index, all_std_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_mods_pos(std, dlt):\n",
    "\n",
    "    filter = re.compile('[^가-힣 +]')\n",
    "\n",
    "    dlt = filter.sub('', dlt)\n",
    "    std = filter.sub('', std)\n",
    "    \n",
    "    \n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "\n",
    "    accessKey = \"7f039a3d-31ea-40ec-938e-4bbfbb0fbc16\"\n",
    "    analysisCode = \"dparse\"\n",
    "\n",
    "    text = std\n",
    "    \n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    tmp = ast.literal_eval(response.data.decode('utf-8'))\n",
    "    dlt = dlt.split()\n",
    "    std = std.split()\n",
    "    words = []\n",
    "    poses = []\n",
    "    # print(std)\n",
    "    #print(tmp[\"return_object\"][\"sentence\"][0][\"dependency\"])\n",
    "    try : \n",
    "        for i in range(len(tmp[\"return_object\"][\"sentence\"][0][\"dependency\"])):\n",
    "            dict_with_mods = tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][i][\"mod\"]\n",
    "            #print(tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][i][\"text\"])\n",
    "            for m in dict_with_mods:\n",
    "                #print(\"std\",std[i])\n",
    "                if std[i] != dlt[i]:\n",
    "                    words.append(eojeol_dict[dlt[i]+\"0\"])\n",
    "                    poses.append(pos_dict[tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][int(m)][\"label\"]])\n",
    "\n",
    "                    words.append(eojeol_dict[std[i]])\n",
    "                    poses.append(pos_dict[tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][int(m)][\"label\"]])\n",
    "                else:\n",
    "         \n",
    "                    words.append(eojeol_dict[std[i]])\n",
    "                    #print(tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][int(m)][\"label\"])\n",
    "                    poses.append(pos_dict[tmp[\"return_object\"][\"sentence\"][0][\"dependency\"][int(m)][\"label\"]])\n",
    "    except:\n",
    "        return [],[]\n",
    "\n",
    "    #print(words, poses)\n",
    "    return words, poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 인덱스와 연결된 단어의 품사 인덱스 \n",
    "def eojeol_dependence(raw_data):\n",
    "    words=[]\n",
    "    poses=[]\n",
    "\n",
    "    for i in raw_data[\"utterance\"]:\n",
    "\n",
    "        std = i[\"standard_form\"]\n",
    "        dlt = i[\"dialect_form\"]\n",
    "        #print(i, std, dlt)\n",
    "        word, pos = get_mods_pos(std, dlt)\n",
    "        words.extend(word)\n",
    "        poses.extend(pos)\n",
    "\n",
    "    df = pd.DataFrame(data = {\"pos\":poses, \"word\":words })\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = eojeol_dependence(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>167747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>146464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>146464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>146464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>169402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54</td>\n",
       "      <td>136476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54</td>\n",
       "      <td>121467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos    word\n",
       "0   59     727\n",
       "1   54     727\n",
       "2   54     727\n",
       "3   54  167747\n",
       "4    1  146464\n",
       "5   56  146464\n",
       "6    3  146464\n",
       "7    8  169402\n",
       "8   54  136476\n",
       "9   54  121467"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169412"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_df[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 인덱스와 연결된 단어의 품사 인덱스\n",
    "#def eojeol_dependence_():\n",
    "\n",
    "    # 언어 분석 기술(문어)\n",
    "    # openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    # accessKey = \"aba3a45d-3318-4061-8a5d-799c8521b082\"\n",
    "    # analysisCode = \"dparse\"\n",
    "\n",
    "    # text = \"엑소브레인은 내 몸 바깥에 있는 인공 두뇌라는 뜻으로, 세계 최고인공지능 기술 선도라는 비전을 달성하기 위한 과학기술정보통신부 소프트웨어 분야의 국가 혁신기술 개발형 연구개발 과제이다.\"\n",
    "    # requestJson = {\n",
    "    #     \"access_key\": accessKey,\n",
    "    #     \"argument\": {\n",
    "    #         \"text\": text,\n",
    "    #         \"analysis_code\": analysisCode\n",
    "    #     }\n",
    "    # }\n",
    "    # http = urllib3.PoolManager()\n",
    "    # response = http.request(\n",
    "    #     \"POST\",\n",
    "    #     openApiURL,\n",
    "    #     headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "    #     body=json.dumps(requestJson)\n",
    "    # )\n",
    "\n",
    "    # s=ast.literal_eval(response.data.decode('utf-8'))\n",
    "\n",
    "    # dep=s['return_object']['sentence'][0]['dependency']\n",
    "    \n",
    "    # eojeol_d=[]\n",
    "    # pos_d=[]\n",
    "\n",
    "    # re_eojeol_dict = {v:k for k,v in eojeol_dict.items()} #{인덱스:문자}\n",
    "        \n",
    "    # for i in range(len(dep)): #dep(딕셔너리)길이만큼 반복 23\n",
    "    #     if dep[i]['text'] in eojeol_dict:\n",
    "    #         eojeol_d.append(eojeol_dict.values())\n",
    "    #     for j in range(len(dep[i]['mod'])):\n",
    "    #         mod=dep[i]['mod']\n",
    "    #         print(mod[j])\n",
    "    #         if len(mod[j]) > 0:\n",
    "    #             pos=re_eojeol_dict.get(mod[j])#결과값:문자\n",
    "    #             #mod가 가리키는 어절의 품사를 확인하는 방법..\n",
    "    #             if dep[i]['label'] in pos_dict:\n",
    "    #                 pos_d.append(pos_dict.values())\n",
    "    #             else:\n",
    "    #                 return None\n",
    "                \n",
    "    # result = pd.DataFrame(data = {\"word_dic\":eojeol_d, \"mod_dic\":pos_d})\n",
    "    # return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 25.7 GiB for an array with shape (40793, 169413) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6c343caa5d95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msynonym_xtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msynonym_ytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynonym_xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 25.7 GiB for an array with shape (40793, 169413) and data type float32"
     ]
    }
   ],
   "source": [
    "synonym_xtrain = np_utils.to_categorical(train_df[\"pos\"])\n",
    "synonym_ytrain = np_utils.to_categorical(train_df[\"word\"])\n",
    "len(synonym_xtrain[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2894, 2989)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_xtrain.shape\n",
    "synonym_ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_xtrain = np_utils.to_categorical(train_df[\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40793, 61)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([40793, 1000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_model = Sequential()\n",
    "# synonym_model.add(Embedding(169412, 150, input_length=40793))\n",
    "synonym_model.add(Dense(512, input_dim = 61, activation=\"relu\"))\n",
    "synonym_model.add(Dense(256,activation=\"relu\"))\n",
    "synonym_model.add(Dense(128,activation=\"relu\"))\n",
    "synonym_model.add(Dense(1000, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "82/82 [==============================] - 5s 39ms/step - loss: -4485.1309 - accuracy: 9.8056e-04\n",
      "Epoch 2/100\n",
      "82/82 [==============================] - 3s 41ms/step - loss: -508698.0312 - accuracy: 0.0013\n",
      "Epoch 3/100\n",
      "82/82 [==============================] - 3s 41ms/step - loss: -5983371.0000 - accuracy: 8.0896e-04\n",
      "Epoch 4/100\n",
      "82/82 [==============================] - 4s 45ms/step - loss: -28978180.0000 - accuracy: 0.0010\n",
      "Epoch 5/100\n",
      "82/82 [==============================] - 4s 44ms/step - loss: -90147984.0000 - accuracy: 0.0012\n",
      "Epoch 6/100\n",
      "82/82 [==============================] - 3s 42ms/step - loss: -215801792.0000 - accuracy: 8.8250e-04\n",
      "Epoch 7/100\n",
      "82/82 [==============================] - 4s 43ms/step - loss: -436835680.0000 - accuracy: 0.0015\n",
      "Epoch 8/100\n",
      "82/82 [==============================] - 3s 42ms/step - loss: -788206080.0000 - accuracy: 0.0012\n",
      "Epoch 9/100\n",
      "82/82 [==============================] - 4s 43ms/step - loss: -1307302656.0000 - accuracy: 0.0013\n",
      "Epoch 10/100\n",
      "82/82 [==============================] - 3s 38ms/step - loss: -2036774272.0000 - accuracy: 0.0012\n",
      "Epoch 11/100\n",
      "82/82 [==============================] - 4s 46ms/step - loss: -3018592256.0000 - accuracy: 0.0016\n",
      "Epoch 12/100\n",
      "82/82 [==============================] - 4s 43ms/step - loss: -4294627584.0000 - accuracy: 8.0896e-04\n",
      "Epoch 13/100\n",
      "82/82 [==============================] - 3s 42ms/step - loss: -5901819392.0000 - accuracy: 0.0013\n",
      "Epoch 14/100\n",
      "82/82 [==============================] - 3s 42ms/step - loss: -7884336640.0000 - accuracy: 0.0011\n",
      "Epoch 15/100\n",
      "82/82 [==============================] - 3s 41ms/step - loss: -10296565760.0000 - accuracy: 9.3153e-04\n",
      "Epoch 16/100\n",
      "82/82 [==============================] - 3s 38ms/step - loss: -13186382848.0000 - accuracy: 8.8250e-04\n",
      "Epoch 17/100\n",
      "82/82 [==============================] - 3s 35ms/step - loss: -16585465856.0000 - accuracy: 9.5605e-04\n",
      "Epoch 18/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -20539238400.0000 - accuracy: 0.0011\n",
      "Epoch 19/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -25127723008.0000 - accuracy: 9.5605e-04\n",
      "Epoch 20/100\n",
      "82/82 [==============================] - 3s 35ms/step - loss: -30347624448.0000 - accuracy: 0.0011\n",
      "Epoch 21/100\n",
      "82/82 [==============================] - 3s 33ms/step - loss: -36283211776.0000 - accuracy: 9.0702e-04\n",
      "Epoch 22/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -42948702208.0000 - accuracy: 7.8445e-04\n",
      "Epoch 23/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -50395361280.0000 - accuracy: 9.3153e-04\n",
      "Epoch 24/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -58662330368.0000 - accuracy: 0.0012\n",
      "Epoch 25/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -67837861888.0000 - accuracy: 0.0012\n",
      "Epoch 26/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -77895917568.0000 - accuracy: 0.0012\n",
      "Epoch 27/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -88900108288.0000 - accuracy: 9.3153e-04\n",
      "Epoch 28/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -100899782656.0000 - accuracy: 0.0012\n",
      "Epoch 29/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -113958780928.0000 - accuracy: 0.0011\n",
      "Epoch 30/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -128113770496.0000 - accuracy: 0.0013\n",
      "Epoch 31/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -143424323584.0000 - accuracy: 9.0702e-04\n",
      "Epoch 32/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -159820054528.0000 - accuracy: 0.0013\n",
      "Epoch 33/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -177526046720.0000 - accuracy: 9.8056e-04\n",
      "Epoch 34/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -196410998784.0000 - accuracy: 0.0010\n",
      "Epoch 35/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -216540839936.0000 - accuracy: 0.0012\n",
      "Epoch 36/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -238047461376.0000 - accuracy: 0.0011\n",
      "Epoch 37/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -260976033792.0000 - accuracy: 0.0014\n",
      "Epoch 38/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -285266804736.0000 - accuracy: 0.0010\n",
      "Epoch 39/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -310910222336.0000 - accuracy: 0.0011\n",
      "Epoch 40/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -338085445632.0000 - accuracy: 0.0010\n",
      "Epoch 41/100\n",
      "82/82 [==============================] - 2s 29ms/step - loss: -366821965824.0000 - accuracy: 0.0012\n",
      "Epoch 42/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -397075021824.0000 - accuracy: 0.0011\n",
      "Epoch 43/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -428967985152.0000 - accuracy: 0.0010\n",
      "Epoch 44/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -462558494720.0000 - accuracy: 0.0015\n",
      "Epoch 45/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -497772167168.0000 - accuracy: 0.0013\n",
      "Epoch 46/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -534601302016.0000 - accuracy: 8.3348e-04\n",
      "Epoch 47/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -573182181376.0000 - accuracy: 0.0013\n",
      "Epoch 48/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -613698895872.0000 - accuracy: 0.0012\n",
      "Epoch 49/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -656044261376.0000 - accuracy: 0.0014\n",
      "Epoch 50/100\n",
      "82/82 [==============================] - 2s 28ms/step - loss: -700292923392.0000 - accuracy: 0.0012\n",
      "Epoch 51/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -746361323520.0000 - accuracy: 9.5605e-04\n",
      "Epoch 52/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -794538737664.0000 - accuracy: 7.8445e-04\n",
      "Epoch 53/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -844557778944.0000 - accuracy: 0.0010\n",
      "Epoch 54/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -896596967424.0000 - accuracy: 0.0013\n",
      "Epoch 55/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -950861758464.0000 - accuracy: 8.3348e-04\n",
      "Epoch 56/100\n",
      "82/82 [==============================] - 3s 33ms/step - loss: -1007239299072.0000 - accuracy: 9.8056e-04\n",
      "Epoch 57/100\n",
      "82/82 [==============================] - 3s 34ms/step - loss: -1065851289600.0000 - accuracy: 0.0012\n",
      "Epoch 58/100\n",
      "82/82 [==============================] - 3s 34ms/step - loss: -1126614433792.0000 - accuracy: 0.0012\n",
      "Epoch 59/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1189568184320.0000 - accuracy: 0.0011\n",
      "Epoch 60/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -1254828146688.0000 - accuracy: 8.8250e-04\n",
      "Epoch 61/100\n",
      "82/82 [==============================] - 3s 37ms/step - loss: -1322559471616.0000 - accuracy: 0.0011\n",
      "Epoch 62/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1392455843840.0000 - accuracy: 9.0702e-04\n",
      "Epoch 63/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1464912314368.0000 - accuracy: 9.3153e-04\n",
      "Epoch 64/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1539712614400.0000 - accuracy: 9.8056e-04\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1617071308800.0000 - accuracy: 0.0011\n",
      "Epoch 66/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1697148960768.0000 - accuracy: 0.0013\n",
      "Epoch 67/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -1779441598464.0000 - accuracy: 0.0011\n",
      "Epoch 68/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -1864692531200.0000 - accuracy: 0.0011\n",
      "Epoch 69/100\n",
      "82/82 [==============================] - 3s 30ms/step - loss: -1952644595712.0000 - accuracy: 0.0013\n",
      "Epoch 70/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2043065139200.0000 - accuracy: 8.0896e-04\n",
      "Epoch 71/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2136516984832.0000 - accuracy: 0.0014\n",
      "Epoch 72/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2232536530944.0000 - accuracy: 0.0012\n",
      "Epoch 73/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2331674935296.0000 - accuracy: 0.0011\n",
      "Epoch 74/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -2433615396864.0000 - accuracy: 0.0012\n",
      "Epoch 75/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2538420305920.0000 - accuracy: 0.0011\n",
      "Epoch 76/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2646417604608.0000 - accuracy: 0.0012\n",
      "Epoch 77/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2757091917824.0000 - accuracy: 0.0011\n",
      "Epoch 78/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -2871102799872.0000 - accuracy: 9.5605e-04\n",
      "Epoch 79/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -2988213534720.0000 - accuracy: 0.0012\n",
      "Epoch 80/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -3108463181824.0000 - accuracy: 0.0013\n",
      "Epoch 81/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -3231994347520.0000 - accuracy: 0.0014\n",
      "Epoch 82/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -3359050563584.0000 - accuracy: 7.8445e-04\n",
      "Epoch 83/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -3488790085632.0000 - accuracy: 0.0011\n",
      "Epoch 84/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -3622752223232.0000 - accuracy: 9.0702e-04\n",
      "Epoch 85/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -3759575924736.0000 - accuracy: 9.3153e-04\n",
      "Epoch 86/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -3899986018304.0000 - accuracy: 9.8056e-04\n",
      "Epoch 87/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -4043922997248.0000 - accuracy: 0.0011\n",
      "Epoch 88/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -4191599722496.0000 - accuracy: 8.3348e-04\n",
      "Epoch 89/100\n",
      "82/82 [==============================] - 3s 34ms/step - loss: -4342815916032.0000 - accuracy: 0.0013\n",
      "Epoch 90/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -4497716805632.0000 - accuracy: 9.0702e-04\n",
      "Epoch 91/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -4656060170240.0000 - accuracy: 9.5605e-04\n",
      "Epoch 92/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -4818651316224.0000 - accuracy: 0.0010\n",
      "Epoch 93/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -4984437473280.0000 - accuracy: 8.0896e-04\n",
      "Epoch 94/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -5154719399936.0000 - accuracy: 0.0011\n",
      "Epoch 95/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -5328706469888.0000 - accuracy: 0.0014\n",
      "Epoch 96/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -5506436956160.0000 - accuracy: 0.0011\n",
      "Epoch 97/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -5688612880384.0000 - accuracy: 0.0011\n",
      "Epoch 98/100\n",
      "82/82 [==============================] - 2s 30ms/step - loss: -5874831065088.0000 - accuracy: 0.0011\n",
      "Epoch 99/100\n",
      "82/82 [==============================] - 3s 32ms/step - loss: -6064979312640.0000 - accuracy: 8.8250e-04\n",
      "Epoch 100/100\n",
      "82/82 [==============================] - 3s 31ms/step - loss: -6259341787136.0000 - accuracy: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c54e617470>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_model.fit(synonym_xtrain, result, batch_size=500, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_model.predict(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동의어 처리를 포함한 사투리 번역 함수\n",
    "def predict_synonym(sentence):\n",
    "    global index_to_standard\n",
    "    res = \"\"\n",
    "    sentence_sep = sentence.split()\n",
    "    #print(sentence_sep)\n",
    "    for i in range(len(sentence_sep)):\n",
    "        if exist_in_dialect(sentence_sep[i]) == True: #사투리 사전에 있으면\n",
    "            if sentence_sep[i] in eojeol_dict.keys(): #해당 단어가 전체 사전에 있으면\n",
    "\n",
    "                pos, _ = get_mods(sentence, i) #연관된 단어의 품사 받아옴\n",
    "                pos_idx = pos_dict[pos] #품사의 인덱스\n",
    "                word = sentence_sep[i] \n",
    "                word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                word_idx = eojeol_dict[word] #단어의 인덱스\n",
    "                # print(word_idx)\n",
    "                # temp = result[word_idx]\n",
    "                # print(type(temp))\n",
    "\n",
    "                one_hot = np_utils.to_categorical(pos_idx,len(synonym_xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                print(type(one_hot))\n",
    "                percentage = synonym_model.predict(np.array([one_hot])) # 동의어 처리 모델\n",
    "                print(percentage[0])\n",
    "                res_idx = max(percentage[word0_idx], percentage[word_idx]) # 둘 중 더 큰 확률값\n",
    "                # print()\n",
    "                one_hot = np_utils.to_categorical(res_idx,len(trans_xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "\n",
    "                pred_index = np.argmax(translate_model.predict(np.array([one_hot]))) # 번역 모델\n",
    "\n",
    "                \n",
    "                # print(\"번역완료\")\n",
    "                # print(\"\")\n",
    "\n",
    "                res = res + \" \" + index_to_standard[pred_index]\n",
    "            else: #해당 단어가 전체 사전에 없으면 -> 동의어가 없으므로 바로 번역 모델에 넣음\n",
    "                \n",
    "                word = sentence_sep[i]\n",
    "                #print(word,\"동의어가 없으므로 바로 번역 모델에 넣음\")\n",
    "                # word0_idx = eojeol_dict[word+\"0\"] #단어0의 인덱스\n",
    "                # print(word0_idx)\n",
    "                # one_hot = np_utils.to_categorical(word0_idx,len(trans_xtrain[0])) #인덱스 배열의크기 지정 필요 size_of_dialect\n",
    "                # pred_index = np.argmax(translate_model.predict(np.array([one_hot])))\n",
    "                res = res + \" \" + translate(word)\n",
    "        else:\n",
    "            word = sentence_sep[i]\n",
    "            #print(word,\"사투리가아님\")\n",
    "            res = res + \" \" + word\n",
    "            \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-f6bd6a3ea402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"했다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1728\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\name\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    894\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "translate_model.predict(\"했다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 37 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-0124f8ac1c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_synonym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"니처럼 어린 아이가 무슨 운전 면허증을 따노\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-118-38d930414b39>\u001b[0m in \u001b[0;36mpredict_synonym\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mpercentage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynonym_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 동의어 처리 모델\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mres_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword0_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 둘 중 더 큰 확률값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                 \u001b[1;31m# print()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mone_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#인덱스 배열의크기 지정 필요 size_of_dialect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 37 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "predict_synonym(\"니처럼 어린 아이가 무슨 운전 면허증을 따노\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 연관된 단어의 품사 추출\n",
    "def get_mods(sentence, num):\n",
    "    #print(sentence, num)\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "\n",
    "    accessKey = \"bd6d2bff-4e44-4035-8591-947fda262051\"\n",
    "    analysisCode = \"dparse\"\n",
    "\n",
    "    text = sentence\n",
    "    \n",
    "    requestJson = {\n",
    "        \"access_key\": accessKey,\n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\"},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    tmp = ast.literal_eval(response.data.decode('utf-8'))\n",
    "    #print(tmp)\n",
    "    dict_with_mods = tmp[\"return_object\"][\"sentence\"][0][\"dependency\"]\n",
    "    id_dict={}\n",
    "    #print(dict_with_mods)\n",
    "    for i in dict_with_mods[num][\"mod\"]:\n",
    "        id_dict[dict_with_mods[int(i)][\"weight\"]] = int(i)\n",
    "        #print(i)\n",
    "    id = id_dict[max(id_dict)]\n",
    "    \n",
    "    pos = dict_with_mods[id][\"label\"]\n",
    "\n",
    "    \n",
    "    return pos, dict_with_mods[id][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나 터지고 나서 배달을 한다 4\n",
      "{'request_id': '', 'result': -1, 'reason': 'Daily amount Limit has been exceed!:6049'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'return_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-1afe5dee14d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_synonym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"코로나 터지고 나서 배달을 한다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-87-9eea85773cf2>\u001b[0m in \u001b[0;36mpredict_synonym\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msentence_sep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meojeol_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#해당 단어가 전체 사전에 있으면\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#연관된 단어의 품사 받아옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mpos_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#품사의 인덱스\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_sep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-bdd0cb463c74>\u001b[0m in \u001b[0;36mget_mods\u001b[1;34m(sentence, num)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdict_with_mods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_object\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dependency\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mid_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_with_mods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'return_object'"
     ]
    }
   ],
   "source": [
    "predict_synonym(\"코로나 터지고 나서 배달을 한다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하지'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"한다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in eojeol_dict.keys():\n",
    "#     for j in eojeol_dict.keys():\n",
    "#         if i[-1]==\"0\":\n",
    "#             tmp=i[:-1]\n",
    "#             if tmp==j:\n",
    "#                 print(tmp, j)\n",
    "\n",
    "eojeol_dict[\"한다0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'hellokakao.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-636-b7372db327b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 호출하여 불러온 결과값을 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hellokakao.mp3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'hellokakao.mp3'"
     ]
    }
   ],
   "source": [
    "import os, requests, json, wave\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "\n",
    "url = \"https://kakaoi-newtone-openapi.kakao.com/v1/synthesize\"\n",
    "key = '8d71d9fd039648d202453dcfaf984f88'\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/xml\",\n",
    "    \"Authorization\": \"KakaoAK \" + key,\n",
    "}\n",
    "text = \"안녕하세요. 아오 왜 저장이 안될까\"\n",
    "\n",
    "# 입력한 텍스트를 변환하여 api를 호출\n",
    "\n",
    "data = f\"<speak>{text}</speak>\".encode('utf-8').decode('latin1')\n",
    "res = requests.post(url, headers = headers, data = data)\n",
    "\n",
    "# 호출하여 불러온 결과값을 저장\n",
    "with open('hellokakao.mp3', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Event(4352-AudioDeviceAdded {'which': 1, 'iscapture': 0})>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pygame, wave\n",
    "\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(\"hellokakao.mp3\")\n",
    "pygame.mixer.music.play()\n",
    "pygame.event.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: Microsoft Sound Mapper - Input  INDEX:  0  RATE:  44100 \n",
      "DEVICE: ¸¶ÀÌÅ©(Synaptics SmartAudio HD)  INDEX:  1  RATE:  44100 \n",
      "DEVICE: Microsoft Sound Mapper - Output  INDEX:  2  RATE:  44100 \n",
      "DEVICE: ½ºÇÇÄ¿(Synaptics SmartAudio HD)  INDEX:  3  RATE:  44100 \n",
      "DEVICE: S23C340(ÀÎÅÚ(R) µð½ºÇÃ·¹ÀÌ ¿Àµð  INDEX:  4  RATE:  44100 \n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "po = pyaudio.PyAudio()\n",
    "\n",
    "for index in range(po.get_device_count()): \n",
    "\n",
    "    desc = po.get_device_info_by_index(index)\n",
    "\n",
    "    #if desc[\"name\"] == \"record\":\n",
    "\n",
    "    print (\"DEVICE: %s  INDEX:  %s  RATE:  %s \" %(desc[\"name\"], index,  int(desc[\"defaultSampleRate\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[727,\n",
       " 727,\n",
       " 727,\n",
       " 167747,\n",
       " 146464,\n",
       " 146464,\n",
       " 146464,\n",
       " 169402,\n",
       " 136476,\n",
       " 121467,\n",
       " 168987,\n",
       " 168987,\n",
       " 741,\n",
       " 741,\n",
       " 741,\n",
       " 168296,\n",
       " 169286,\n",
       " 160615,\n",
       " 156915,\n",
       " 156915,\n",
       " 156915,\n",
       " 156915,\n",
       " 168802,\n",
       " 163506,\n",
       " 163506,\n",
       " 163506,\n",
       " 120517,\n",
       " 151508,\n",
       " 151508,\n",
       " 162721,\n",
       " 166632,\n",
       " 168939,\n",
       " 168939,\n",
       " 168939,\n",
       " 169397,\n",
       " 150806,\n",
       " 150806,\n",
       " 150806,\n",
       " 169402,\n",
       " 136476,\n",
       " 165549,\n",
       " 774,\n",
       " 774,\n",
       " 774,\n",
       " 121467,\n",
       " 159467,\n",
       " 159467,\n",
       " 159467,\n",
       " 159467,\n",
       " 168889,\n",
       " 168889,\n",
       " 168889,\n",
       " 152296,\n",
       " 785,\n",
       " 133902,\n",
       " 133902,\n",
       " 164572,\n",
       " 164572,\n",
       " 164572,\n",
       " 164572,\n",
       " 164572,\n",
       " 169224,\n",
       " 161307,\n",
       " 166830,\n",
       " 166830,\n",
       " 166830,\n",
       " 152296,\n",
       " 169149,\n",
       " 131562,\n",
       " 804,\n",
       " 169159,\n",
       " 168889,\n",
       " 168889,\n",
       " 152296,\n",
       " 154674,\n",
       " 154674,\n",
       " 814,\n",
       " 814,\n",
       " 814,\n",
       " 164423,\n",
       " 164423,\n",
       " 164423,\n",
       " 164423,\n",
       " 169332,\n",
       " 164777,\n",
       " 152415,\n",
       " 825,\n",
       " 825,\n",
       " 169150,\n",
       " 168889,\n",
       " 168889,\n",
       " 168890,\n",
       " 161223,\n",
       " 161223,\n",
       " 162737,\n",
       " 168889,\n",
       " 168890,\n",
       " 153767,\n",
       " 837,\n",
       " 151219,\n",
       " 151219,\n",
       " 151219,\n",
       " 150713,\n",
       " 43386,\n",
       " 166302,\n",
       " 166302,\n",
       " 168004,\n",
       " 168004,\n",
       " 167306,\n",
       " 167306,\n",
       " 167306,\n",
       " 167306,\n",
       " 167306,\n",
       " 168889,\n",
       " 168889,\n",
       " 168889,\n",
       " 162611,\n",
       " 156057,\n",
       " 156057,\n",
       " 156057,\n",
       " 156057,\n",
       " 156507,\n",
       " 866,\n",
       " 166758,\n",
       " 144072,\n",
       " 144072,\n",
       " 144072,\n",
       " 900,\n",
       " 168007,\n",
       " 875,\n",
       " 900,\n",
       " 900,\n",
       " 126238,\n",
       " 126238,\n",
       " 169402,\n",
       " 163787,\n",
       " 163787,\n",
       " 163787,\n",
       " 169225,\n",
       " 93584,\n",
       " 93584,\n",
       " 93584,\n",
       " 166630,\n",
       " 169286,\n",
       " 169273,\n",
       " 169223,\n",
       " 893,\n",
       " 893,\n",
       " 166758,\n",
       " 137890,\n",
       " 137890,\n",
       " 137890,\n",
       " 137890,\n",
       " 900,\n",
       " 144072,\n",
       " 169105,\n",
       " 169225,\n",
       " 168301,\n",
       " 168301,\n",
       " 168301,\n",
       " 152296,\n",
       " 57450,\n",
       " 57450,\n",
       " 163791,\n",
       " 166611,\n",
       " 168056,\n",
       " 918,\n",
       " 918,\n",
       " 918,\n",
       " 918,\n",
       " 918,\n",
       " 153505,\n",
       " 153505,\n",
       " 153505,\n",
       " 153505,\n",
       " 161179,\n",
       " 167986,\n",
       " 167986,\n",
       " 167986,\n",
       " 167986,\n",
       " 157237,\n",
       " 157237,\n",
       " 120831,\n",
       " 120831,\n",
       " 169225,\n",
       " 158057,\n",
       " 158057,\n",
       " 166831,\n",
       " 973,\n",
       " 169305,\n",
       " 169305,\n",
       " 169305,\n",
       " 169305,\n",
       " 169305,\n",
       " 169305,\n",
       " 111429,\n",
       " 108467,\n",
       " 108467,\n",
       " 169372,\n",
       " 169372,\n",
       " 169372,\n",
       " 152296,\n",
       " 166630,\n",
       " 166512,\n",
       " 957,\n",
       " 960,\n",
       " 960,\n",
       " 960,\n",
       " 964,\n",
       " 965,\n",
       " 965,\n",
       " 100821,\n",
       " 100821,\n",
       " 969,\n",
       " 168466,\n",
       " 973,\n",
       " 973,\n",
       " 973,\n",
       " 163639,\n",
       " 169270,\n",
       " 159879,\n",
       " 159879,\n",
       " 166831,\n",
       " 985,\n",
       " 985,\n",
       " 169105,\n",
       " 169105,\n",
       " 169105,\n",
       " 154032,\n",
       " 168301,\n",
       " 168301,\n",
       " 166831,\n",
       " 162793,\n",
       " 166039,\n",
       " 166039,\n",
       " 166039,\n",
       " 119368,\n",
       " 169225,\n",
       " 1000,\n",
       " 1000,\n",
       " 152296,\n",
       " 120393,\n",
       " 120393,\n",
       " 120393,\n",
       " 120393,\n",
       " 120393,\n",
       " 10311,\n",
       " 167279,\n",
       " 167279,\n",
       " 167279,\n",
       " 167279,\n",
       " 167279,\n",
       " 119568,\n",
       " 1750,\n",
       " 83290,\n",
       " 1026,\n",
       " 1026,\n",
       " 1026,\n",
       " 1026,\n",
       " 1026,\n",
       " 159872,\n",
       " 159872,\n",
       " 159872,\n",
       " 157848,\n",
       " 1035,\n",
       " 1035,\n",
       " 1035,\n",
       " 1035,\n",
       " 1036,\n",
       " 158732,\n",
       " 152852,\n",
       " 168695,\n",
       " 1046,\n",
       " 1046,\n",
       " 1046,\n",
       " 1046,\n",
       " 1046,\n",
       " 169270,\n",
       " 59303,\n",
       " 59303,\n",
       " 59303,\n",
       " 59303,\n",
       " 1054,\n",
       " 1058,\n",
       " 1058,\n",
       " 1058,\n",
       " 1058,\n",
       " 165829,\n",
       " 165829,\n",
       " 165829,\n",
       " 158145,\n",
       " 1066,\n",
       " 1067,\n",
       " 1067,\n",
       " 1067,\n",
       " 1067,\n",
       " 163388,\n",
       " 1072,\n",
       " 1074,\n",
       " 1074,\n",
       " 1074,\n",
       " 1074,\n",
       " 120007,\n",
       " 1079,\n",
       " 1079,\n",
       " 1079,\n",
       " 31294,\n",
       " 31294,\n",
       " 1086,\n",
       " 1086,\n",
       " 1086,\n",
       " 1086,\n",
       " 119568,\n",
       " 1090,\n",
       " 140860,\n",
       " 151845,\n",
       " 169341,\n",
       " 164558,\n",
       " 164558,\n",
       " 144075,\n",
       " 144075,\n",
       " 144075,\n",
       " 119568,\n",
       " 119568,\n",
       " 167991,\n",
       " 167587,\n",
       " 124206,\n",
       " 1108,\n",
       " 1108,\n",
       " 1108,\n",
       " 167279,\n",
       " 169366,\n",
       " 169366,\n",
       " 169366,\n",
       " 164746,\n",
       " 169402,\n",
       " 163379,\n",
       " 38344,\n",
       " 38344,\n",
       " 38344,\n",
       " 166830,\n",
       " 166830,\n",
       " 152296,\n",
       " 166830,\n",
       " 166830,\n",
       " 1926,\n",
       " 155263,\n",
       " 155263,\n",
       " 1130,\n",
       " 105544,\n",
       " 168389,\n",
       " 1135,\n",
       " 1135,\n",
       " 1135,\n",
       " 1137,\n",
       " 1139,\n",
       " 124368,\n",
       " 124368,\n",
       " 169105,\n",
       " 169105,\n",
       " 156480,\n",
       " 156480,\n",
       " 169410,\n",
       " 169410,\n",
       " 169311,\n",
       " 166830,\n",
       " 166830,\n",
       " 166830,\n",
       " 166830,\n",
       " 166830,\n",
       " 152296,\n",
       " 168144,\n",
       " 168335,\n",
       " 166830,\n",
       " 166830,\n",
       " 168890,\n",
       " 153767,\n",
       " 169155,\n",
       " 169155,\n",
       " 169155,\n",
       " 163071,\n",
       " 166830,\n",
       " 152296,\n",
       " 166630,\n",
       " 166630,\n",
       " 166630,\n",
       " 166512,\n",
       " 163313,\n",
       " 113814,\n",
       " 113814,\n",
       " 1178,\n",
       " 168114,\n",
       " 169228,\n",
       " 169228,\n",
       " 169228,\n",
       " 169228,\n",
       " 169228,\n",
       " 168813,\n",
       " 12447,\n",
       " 163050,\n",
       " 167279,\n",
       " 167306,\n",
       " 167306,\n",
       " 134262,\n",
       " 134262,\n",
       " 134262,\n",
       " 163313,\n",
       " 169129,\n",
       " 164071,\n",
       " 164071,\n",
       " 161960,\n",
       " 161960,\n",
       " 1205,\n",
       " 1205,\n",
       " 1210,\n",
       " 1210,\n",
       " 151504,\n",
       " 1,\n",
       " 169398,\n",
       " 1,\n",
       " 169398,\n",
       " 1,\n",
       " 169398,\n",
       " 1,\n",
       " 169398,\n",
       " 96940,\n",
       " 123892,\n",
       " 169311,\n",
       " 1218,\n",
       " 1218,\n",
       " 1218,\n",
       " 168175,\n",
       " 112087,\n",
       " 112087,\n",
       " 169091,\n",
       " 1226,\n",
       " 1226,\n",
       " 1226,\n",
       " 1228,\n",
       " 169386,\n",
       " 169386,\n",
       " 1231,\n",
       " 165681,\n",
       " 165681,\n",
       " 165681,\n",
       " 161960,\n",
       " 161960,\n",
       " 168958,\n",
       " 161527,\n",
       " 151070,\n",
       " 151070,\n",
       " 151070,\n",
       " 155060,\n",
       " 1246,\n",
       " 169042,\n",
       " 169311,\n",
       " 167351,\n",
       " 167351,\n",
       " 169311,\n",
       " 165019,\n",
       " 1255,\n",
       " 169311,\n",
       " 154069,\n",
       " 154069,\n",
       " 1261,\n",
       " 1261,\n",
       " 1261,\n",
       " 161960,\n",
       " 161960,\n",
       " 166782,\n",
       " 166782,\n",
       " 166782,\n",
       " 166782,\n",
       " 162843,\n",
       " 70058,\n",
       " 1273,\n",
       " 1273,\n",
       " 1273,\n",
       " 168515,\n",
       " 1277,\n",
       " 1820,\n",
       " 1820,\n",
       " 168987,\n",
       " 1284,\n",
       " 1284,\n",
       " 1284,\n",
       " 1284,\n",
       " 168346,\n",
       " 161831,\n",
       " 160724,\n",
       " 160724,\n",
       " 1311,\n",
       " 91806,\n",
       " 91806,\n",
       " 160724,\n",
       " 1311,\n",
       " 133047,\n",
       " 133047,\n",
       " 137071,\n",
       " 1302,\n",
       " 1302,\n",
       " 1302,\n",
       " 161831,\n",
       " 1311,\n",
       " 91806,\n",
       " 91806,\n",
       " 91806,\n",
       " 91806,\n",
       " 1311,\n",
       " 133047,\n",
       " 133047,\n",
       " 133047,\n",
       " 168699,\n",
       " 167381,\n",
       " 1318,\n",
       " 1319,\n",
       " 1319,\n",
       " 1320,\n",
       " 86383,\n",
       " 102684,\n",
       " 1324,\n",
       " 169228,\n",
       " 169228,\n",
       " 169228,\n",
       " 73853,\n",
       " 153973,\n",
       " 153973,\n",
       " 1334,\n",
       " 1334,\n",
       " 1334,\n",
       " 1340,\n",
       " 1340,\n",
       " 169389,\n",
       " 169389,\n",
       " 169389,\n",
       " 169389,\n",
       " 1343,\n",
       " 164288,\n",
       " 164288,\n",
       " 159872,\n",
       " 159872,\n",
       " 168960,\n",
       " 168960,\n",
       " 168960,\n",
       " 1355,\n",
       " 1355,\n",
       " 1355,\n",
       " 1355,\n",
       " 153598,\n",
       " 160724,\n",
       " 160724,\n",
       " 160724,\n",
       " 160724,\n",
       " 169402,\n",
       " 136476,\n",
       " 165684,\n",
       " 165684,\n",
       " 165684,\n",
       " 169411,\n",
       " 1372,\n",
       " 1372,\n",
       " 1372,\n",
       " 100728,\n",
       " 100728,\n",
       " 100728,\n",
       " 169402,\n",
       " 136476,\n",
       " 169144,\n",
       " 140984,\n",
       " 167209,\n",
       " 166623,\n",
       " 105947,\n",
       " 105947,\n",
       " 105947,\n",
       " 105947,\n",
       " 169402,\n",
       " 168913,\n",
       " 121783,\n",
       " 121783,\n",
       " 121783,\n",
       " 38783,\n",
       " 38783,\n",
       " 169018,\n",
       " 169018,\n",
       " 169105,\n",
       " 169105,\n",
       " 159674,\n",
       " 168890,\n",
       " 1405,\n",
       " 1405,\n",
       " 168055,\n",
       " 168055,\n",
       " 168293,\n",
       " 168717,\n",
       " 168717,\n",
       " 168717,\n",
       " 169386,\n",
       " 169386,\n",
       " 169386,\n",
       " 1417,\n",
       " 1418,\n",
       " 109069,\n",
       " 109069,\n",
       " 109069,\n",
       " 1422,\n",
       " 167279,\n",
       " 167279,\n",
       " 167279,\n",
       " 158999,\n",
       " 169331,\n",
       " 169331,\n",
       " 166611,\n",
       " 168293,\n",
       " 43258,\n",
       " 43258,\n",
       " 166967,\n",
       " 166967,\n",
       " 165447,\n",
       " 161237,\n",
       " 161960,\n",
       " 161960,\n",
       " 120618,\n",
       " 120618,\n",
       " 120618,\n",
       " 120618,\n",
       " 120618,\n",
       " 168296,\n",
       " 166590,\n",
       " 1450,\n",
       " 127833,\n",
       " 1856,\n",
       " 169228,\n",
       " 169228,\n",
       " 169228,\n",
       " 169338,\n",
       " 83290,\n",
       " 166611,\n",
       " 166611,\n",
       " 161766,\n",
       " 38783,\n",
       " 1464,\n",
       " 1465,\n",
       " 118776,\n",
       " 169311,\n",
       " 158732,\n",
       " 158732,\n",
       " 158732,\n",
       " 158732,\n",
       " 124157,\n",
       " 161049,\n",
       " 161049,\n",
       " 161049,\n",
       " 152393,\n",
       " 165058,\n",
       " 169258,\n",
       " 1482,\n",
       " 1482,\n",
       " 1482,\n",
       " 163791,\n",
       " 163791,\n",
       " 163791,\n",
       " 119348,\n",
       " 8270,\n",
       " 8270,\n",
       " 8270,\n",
       " 168889,\n",
       " 168889,\n",
       " 168889,\n",
       " 168889,\n",
       " 168889,\n",
       " 167097,\n",
       " 169374,\n",
       " 169228,\n",
       " 147913,\n",
       " 153394,\n",
       " 153394,\n",
       " 153394,\n",
       " 153394,\n",
       " 153394,\n",
       " 121307,\n",
       " 121307,\n",
       " 167844,\n",
       " 167844,\n",
       " 167844,\n",
       " 156510,\n",
       " 168754,\n",
       " 168754,\n",
       " 166791,\n",
       " 166791,\n",
       " 168067,\n",
       " 163624,\n",
       " 163624,\n",
       " 166310,\n",
       " 166310,\n",
       " 2257,\n",
       " 2257,\n",
       " 28166,\n",
       " 160615,\n",
       " 160615,\n",
       " 160615,\n",
       " 163690,\n",
       " 167506,\n",
       " 169149,\n",
       " 169149,\n",
       " 169149,\n",
       " 166266,\n",
       " 166266,\n",
       " 162667,\n",
       " 162667,\n",
       " 166266,\n",
       " 153781,\n",
       " 168296,\n",
       " 159956,\n",
       " 162486,\n",
       " 162486,\n",
       " 1551,\n",
       " 162611,\n",
       " 101299,\n",
       " 101299,\n",
       " 169311,\n",
       " 169270,\n",
       " 169270,\n",
       " 1559,\n",
       " 1559,\n",
       " 167924,\n",
       " 140860,\n",
       " 1564,\n",
       " 109092,\n",
       " 168293,\n",
       " 168293,\n",
       " 1568,\n",
       " 110366,\n",
       " 160322,\n",
       " 160322,\n",
       " 160322,\n",
       " 160322,\n",
       " 169326,\n",
       " 169326,\n",
       " 110295,\n",
       " 134227,\n",
       " 121186,\n",
       " 121186,\n",
       " 121186,\n",
       " 1584,\n",
       " 167023,\n",
       " 167023,\n",
       " 166684,\n",
       " 157848,\n",
       " 157848,\n",
       " 157848,\n",
       " 1592,\n",
       " 169091,\n",
       " 150852,\n",
       " 150852,\n",
       " 1595,\n",
       " 1597,\n",
       " 169105,\n",
       " 1600,\n",
       " 1600,\n",
       " 169402,\n",
       " 167436,\n",
       " 167436,\n",
       " 167436,\n",
       " 167436,\n",
       " 140967,\n",
       " 140967,\n",
       " 140967,\n",
       " 140967,\n",
       " 168958,\n",
       " 1613,\n",
       " 163579,\n",
       " 163579,\n",
       " 163579,\n",
       " 167587,\n",
       " 148335,\n",
       " 148335,\n",
       " 169015,\n",
       " 134208,\n",
       " 143495,\n",
       " 143495,\n",
       " 143495,\n",
       " 169131,\n",
       " 136476,\n",
       " 121783,\n",
       " 121783,\n",
       " 158213,\n",
       " 158213,\n",
       " 158213,\n",
       " 168813,\n",
       " 163624,\n",
       " 126958,\n",
       " 126958,\n",
       " 126958,\n",
       " 126958,\n",
       " 136656,\n",
       " 168754,\n",
       " 168754,\n",
       " 168754,\n",
       " 166963,\n",
       " 164578,\n",
       " 164578,\n",
       " 169411,\n",
       " 1684,\n",
       " 169270,\n",
       " 88606,\n",
       " 1655,\n",
       " 1655,\n",
       " 169015,\n",
       " 161625,\n",
       " 161625,\n",
       " 1662,\n",
       " 1662,\n",
       " 1662,\n",
       " 169015,\n",
       " 169273,\n",
       " 169273,\n",
       " 169273,\n",
       " 169402,\n",
       " 168913,\n",
       " 169270,\n",
       " 169270,\n",
       " 88606,\n",
       " 156057,\n",
       " 156057,\n",
       " 150140,\n",
       " 169383,\n",
       " 169383,\n",
       " 169383,\n",
       " 169228,\n",
       " 160464,\n",
       " 160464,\n",
       " 169270,\n",
       " 169270,\n",
       " 1687,\n",
       " 1687,\n",
       " 162990,\n",
       " 1695,\n",
       " 1695,\n",
       " 1695,\n",
       " 1695,\n",
       " 1695,\n",
       " 1695,\n",
       " 1696,\n",
       " 168293,\n",
       " 169270,\n",
       " 169270,\n",
       " 145226,\n",
       " 145226,\n",
       " 159604,\n",
       " 159604,\n",
       " 159604,\n",
       " 167587,\n",
       " 161625,\n",
       " 161625,\n",
       " 161625,\n",
       " 161625,\n",
       " 169311,\n",
       " 101513,\n",
       " 101513,\n",
       " 101513,\n",
       " 101513,\n",
       " 110596,\n",
       " 110596,\n",
       " 110596,\n",
       " 60260,\n",
       " 166630,\n",
       " 166630,\n",
       " 168293,\n",
       " 1726,\n",
       " 1726,\n",
       " 109949,\n",
       " 168958,\n",
       " 1733,\n",
       " 1733,\n",
       " 1733,\n",
       " 1733,\n",
       " 100821,\n",
       " 1736,\n",
       " 168104,\n",
       " 168104,\n",
       " 168104,\n",
       " 131683,\n",
       " 165878,\n",
       " 165878,\n",
       " 1743,\n",
       " 169366,\n",
       " 1746,\n",
       " 167279,\n",
       " 167279,\n",
       " 1749,\n",
       " 169294,\n",
       " 169294,\n",
       " 169294,\n",
       " 169294,\n",
       " 149727,\n",
       " 169311,\n",
       " 162486,\n",
       " 162486,\n",
       " 155118,\n",
       " 1765,\n",
       " 169015,\n",
       " 169015,\n",
       " 1768,\n",
       " 1768,\n",
       " 1768,\n",
       " 1768,\n",
       " 147350,\n",
       " 146183,\n",
       " 169235,\n",
       " 167436,\n",
       " 167436,\n",
       " 166153,\n",
       " 166153,\n",
       " 166153,\n",
       " 166153,\n",
       " 1783,\n",
       " 1783,\n",
       " 1783,\n",
       " 1784,\n",
       " 1784,\n",
       " 169114,\n",
       " 124193,\n",
       " 124193,\n",
       " 140967,\n",
       " 136924,\n",
       " 168301,\n",
       " 168301,\n",
       " 164642,\n",
       " 169402,\n",
       " 136476,\n",
       " 136476,\n",
       " 136476,\n",
       " 136476,\n",
       " 165058,\n",
       " 163414,\n",
       " 1806,\n",
       " 1809,\n",
       " 1809,\n",
       " 1809,\n",
       " 1809,\n",
       " 1809,\n",
       " 1809,\n",
       " 1809,\n",
       " 165058,\n",
       " 115910,\n",
       " 169336,\n",
       " 169402,\n",
       " 136476,\n",
       " 136476,\n",
       " 136476,\n",
       " 1819,\n",
       " 169228,\n",
       " 169228,\n",
       " 169338,\n",
       " 1826,\n",
       " 1826,\n",
       " 168802,\n",
       " 1828,\n",
       " 165134,\n",
       " 1832,\n",
       " 168952,\n",
       " 168952,\n",
       " 165134,\n",
       " 169409,\n",
       " 1837,\n",
       " 1837,\n",
       " 1838,\n",
       " 168958,\n",
       " 168398,\n",
       " 168699,\n",
       " 168699,\n",
       " 169228,\n",
       " 169228,\n",
       " 168754,\n",
       " 168754,\n",
       " 168754,\n",
       " 169402,\n",
       " 167202,\n",
       " 167202,\n",
       " 167202,\n",
       " 167202,\n",
       " 169065,\n",
       " 169065,\n",
       " 169228,\n",
       " 169338,\n",
       " 168754,\n",
       " 168754,\n",
       " 168754,\n",
       " 168754,\n",
       " 1864,\n",
       " 1864,\n",
       " 1864,\n",
       " 161640,\n",
       " 167128,\n",
       " 169386,\n",
       " 169386,\n",
       " 72643,\n",
       " 72643,\n",
       " 168104,\n",
       " 163624,\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# len(sentence) = 13\n",
    "sentence=np.array(list(train_df[\"word\"]))\n",
    "\n",
    "W=tf.Variable(\n",
    "    tf.random.uniform([169413,1000], -1.0, 1.0) ,name=\"W\")\n",
    "\n",
    "result=tf.nn.embedding_lookup(W,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([40793, 1000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab2d543e6fcd2c9d1adcb8a11712178d53957602a005263fff7c4bba6d50274d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('name': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
